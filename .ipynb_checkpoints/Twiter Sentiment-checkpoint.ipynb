{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source:\n",
    "\n",
    "http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\n",
    "\n",
    "## Word Embeddings:\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, Conv1D, MaxPool1D, concatenate, Input, Reshape, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Hyper-parameters\n",
    "\n",
    "n_total_sentences = 10000\n",
    "size_vocab = 5000\n",
    "sentence_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Sentiment Analysis Dataset.csv\", \"r\") as f:\n",
    "    sentiment = []\n",
    "    sentences = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0: continue\n",
    "        columns = line.split(\",\")\n",
    "        sentiment.append(columns[1])\n",
    "        sentences.append(columns[3].strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578627\n"
     ]
    }
   ],
   "source": [
    "print(len(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sentences[:n_total_sentences]\n",
    "sentiment = sentiment[:n_total_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is so sad for my apl friend.............\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nlp(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = [w.text for s in tokenized_sentences for w in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135482\n"
     ]
    }
   ],
   "source": [
    "print(len(all_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Words: 17688\n",
      "Average number of times a word appears: 7.659543193125283\n"
     ]
    }
   ],
   "source": [
    "print(\"N Words: {}\".format(len(word_counts)))\n",
    "print(\"Average number of times a word appears: {}\".format(np.mean(list(word_counts.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w[0]: i for i, w in enumerate(word_counts.most_common(size_vocab))}\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_index(sentences):\n",
    "    indexed_sentences = []\n",
    "    for s in sentences:\n",
    "        tmp = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                tmp.append(word2index[w.text])\n",
    "            except:\n",
    "                tmp.append(size_vocab)\n",
    "        indexed_sentences.append(tmp)\n",
    "    return indexed_sentences\n",
    "\n",
    "\n",
    "def indexed_to_words(sentence):\n",
    "    words = []\n",
    "    for index in sentence:\n",
    "        try:\n",
    "            words.append(index2word[index])\n",
    "        except:\n",
    "            words.append(\"<unk>\")\n",
    "    return words\n",
    "\n",
    "sentences_indexed = sentences_to_index(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is so sad for my apl friend............."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'so', 'sad', 'for', 'my', '<unk>', 'friend', '.............']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_to_words(sentences_indexed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(sentences_indexed)) < 0.8\n",
    "trn_sentences = np.array(sentences_indexed)[msk]\n",
    "val_sentences = np.array(sentences_indexed)[~msk]\n",
    "trn_sentiment = np.array(sentiment)[msk]\n",
    "val_sentiment = np.array(sentiment)[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 7948\n",
      "Validation Size: 2052\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Size: {}\".format(len(trn_sentences)))\n",
    "print(\"Validation Size: {}\".format(len(val_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 1, 13.535480624056367)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(list(map(len, trn_sentences)))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sentences = sequence.pad_sequences(trn_sentences, maxlen=sentence_size, value=size_vocab+1)\n",
    "val_sentences = sequence.pad_sequences(val_sentences, maxlen=sentence_size, value=size_vocab+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7948, 25)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(size_vocab+2, 64, input_length=sentence_size),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 25, 64)            320128    \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               160100    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 480,329\n",
      "Trainable params: 480,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7948 samples, validate on 2052 samples\n",
      "Epoch 1/3\n",
      "7948/7948 [==============================] - 0s - loss: 0.6396 - acc: 0.6300 - val_loss: 0.5672 - val_acc: 0.7042\n",
      "Epoch 2/3\n",
      "7948/7948 [==============================] - 0s - loss: 0.4743 - acc: 0.7793 - val_loss: 0.5046 - val_acc: 0.7495\n",
      "Epoch 3/3\n",
      "7948/7948 [==============================] - 0s - loss: 0.3083 - acc: 0.8783 - val_loss: 0.5522 - val_acc: 0.7481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x260fef0b8>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_sentences, trn_sentiment, validation_data=(val_sentences, val_sentiment), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(size_vocab+2, 64, input_length=sentence_size),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPool1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 25, 64)            320128    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 25, 64)            20544     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               76900     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 417,673\n",
      "Trainable params: 417,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "conv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7948 samples, validate on 2052 samples\n",
      "Epoch 1/3\n",
      "7948/7948 [==============================] - 3s - loss: 0.6205 - acc: 0.6429 - val_loss: 0.5336 - val_acc: 0.7456\n",
      "Epoch 2/3\n",
      "7948/7948 [==============================] - 3s - loss: 0.4436 - acc: 0.8016 - val_loss: 0.4979 - val_acc: 0.7558\n",
      "Epoch 3/3\n",
      "7948/7948 [==============================] - 3s - loss: 0.3419 - acc: 0.8621 - val_loss: 0.5106 - val_acc: 0.75190.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x265475f28>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn_sentences, trn_sentiment, validation_data=(val_sentences, val_sentiment), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Size CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_in = Input ((size_vocab+2, 64))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPool1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = concatenate(convs) \n",
    "graph = Model(graph_in, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_cnn = Sequential ([\n",
    "    Embedding(size_vocab+2, 64, input_length=sentence_size),\n",
    "    Dropout (0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 25, 64)            320128    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "model_7 (Model)              multiple                  49344     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 100)               230500    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 600,073\n",
      "Trainable params: 600,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ms_cnn.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "ms_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7948 samples, validate on 2052 samples\n",
      "Epoch 1/3\n",
      "7948/7948 [==============================] - 5s - loss: 0.6194 - acc: 0.6521 - val_loss: 0.5270 - val_acc: 0.7451\n",
      "Epoch 2/3\n",
      "7948/7948 [==============================] - 6s - loss: 0.4440 - acc: 0.8039 - val_loss: 0.4890 - val_acc: 0.7714\n",
      "Epoch 3/3\n",
      "7948/7948 [==============================] - 6s - loss: 0.3439 - acc: 0.8582 - val_loss: 0.5286 - val_acc: 0.7671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26e32d748>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_cnn.fit(trn_sentences, trn_sentiment, validation_data=(val_sentences, val_sentiment), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_sentence_to_index(sentence):\n",
    "    s = nlp(sentence)\n",
    "    ind_s = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            ind_s.append(word2index[w.text])\n",
    "        except:\n",
    "            ind_s.append(size_vocab)\n",
    "    padded = sequence.pad_sequences([ind_s], maxlen=sentence_size, value=size_vocab+1)[0]\n",
    "    return np.expand_dims(padded, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = raw_sentence_to_index(\"I love this movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9045459]], dtype=float32)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_cnn.predict(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02032357]], dtype=float32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx = raw_sentence_to_index(\"The movie seemed good, but in the end, was terrible.\")\n",
    "ms_cnn.predict(indx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
