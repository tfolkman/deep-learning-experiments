{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source:\n",
    "\n",
    "http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\n",
    "\n",
    "## Word Embeddings:\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, Conv1D, MaxPool1D, concatenate, Input, Reshape, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Hyper-parameters\n",
    "\n",
    "n_total_sentences = 10000\n",
    "size_vocab = 5000\n",
    "sentence_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/Sentiment Analysis Dataset.csv\", \"r\") as f:\n",
    "    sentiment = []\n",
    "    sentences = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0: continue\n",
    "        columns = line.split(\",\")\n",
    "        sentiment.append(columns[1])\n",
    "        sentences.append(columns[3].strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578627\n"
     ]
    }
   ],
   "source": [
    "print(len(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sentences[:n_total_sentences]\n",
    "sentiment = sentiment[:n_total_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is so sad for my apl friend.............\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nlp(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vocab = [w.text for s in tokenized_sentences for w in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135482\n"
     ]
    }
   ],
   "source": [
    "print(len(all_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Words: 17688\n",
      "Average number of times a word appears: 7.659543193125283\n"
     ]
    }
   ],
   "source": [
    "print(\"N Words: {}\".format(len(word_counts)))\n",
    "print(\"Average number of times a word appears: {}\".format(np.mean(list(word_counts.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {w[0]: i for i, w in enumerate(word_counts.most_common(size_vocab))}\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_index(sentences):\n",
    "    indexed_sentences = []\n",
    "    for s in sentences:\n",
    "        tmp = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                tmp.append(word2index[w.text])\n",
    "            except:\n",
    "                tmp.append(size_vocab)\n",
    "        indexed_sentences.append(tmp)\n",
    "    return indexed_sentences\n",
    "\n",
    "\n",
    "def indexed_to_words(sentence):\n",
    "    words = []\n",
    "    for index in sentence:\n",
    "        try:\n",
    "            words.append(index2word[index])\n",
    "        except:\n",
    "            words.append(\"<unk>\")\n",
    "    return words\n",
    "\n",
    "sentences_indexed = sentences_to_index(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is so sad for my apl friend............."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'so', 'sad', 'for', 'my', '<unk>', 'friend', '.............']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_to_words(sentences_indexed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(sentences_indexed)) < 0.8\n",
    "trn_sentences = np.array(sentences_indexed)[msk]\n",
    "val_sentences = np.array(sentences_indexed)[~msk]\n",
    "trn_sentiment = np.array(sentiment)[msk]\n",
    "val_sentiment = np.array(sentiment)[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 8045\n",
      "Validation Size: 1955\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Size: {}\".format(len(trn_sentences)))\n",
    "print(\"Validation Size: {}\".format(len(val_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1, 13.462150403977626)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(list(map(len, trn_sentences)))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_sentences = sequence.pad_sequences(trn_sentences, maxlen=sentence_size, value=size_vocab+1)\n",
    "val_sentences = sequence.pad_sequences(val_sentences, maxlen=sentence_size, value=size_vocab+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8045, 25)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(size_vocab+2, 64, input_length=sentence_size),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 64)            320128    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               160100    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 480,329\n",
      "Trainable params: 480,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8045 samples, validate on 1955 samples\n",
      "Epoch 1/3\n",
      "8045/8045 [==============================] - 0s - loss: 0.6364 - acc: 0.6369 - val_loss: 0.5548 - val_acc: 0.7366\n",
      "Epoch 2/3\n",
      "8045/8045 [==============================] - 0s - loss: 0.4708 - acc: 0.7815 - val_loss: 0.5009 - val_acc: 0.7535\n",
      "Epoch 3/3\n",
      "8045/8045 [==============================] - 0s - loss: 0.2976 - acc: 0.8818 - val_loss: 0.5375 - val_acc: 0.7417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118529898>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_sentences, trn_sentiment, validation_data=(val_sentences, val_sentiment), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(size_vocab+2, 64, input_length=sentence_size),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPool1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 64)            320128    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 25, 64)            20544     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               76900     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 417,673\n",
      "Trainable params: 417,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "conv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8045 samples, validate on 1955 samples\n",
      "Epoch 1/3\n",
      "8045/8045 [==============================] - 3s - loss: 0.6287 - acc: 0.6446 - val_loss: 0.5277 - val_acc: 0.7519\n",
      "Epoch 2/3\n",
      "8045/8045 [==============================] - 2s - loss: 0.4471 - acc: 0.7955 - val_loss: 0.4836 - val_acc: 0.7627\n",
      "Epoch 3/3\n",
      "8045/8045 [==============================] - 2s - loss: 0.3454 - acc: 0.8572 - val_loss: 0.5063 - val_acc: 0.7662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f109cf8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn_sentences, trn_sentiment, validation_data=(val_sentences, val_sentiment), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Size CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((size_vocab+2, 64))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPool1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = concatenate(convs) \n",
    "graph = Model(graph_in, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms_cnn = Sequential ([\n",
    "    Embedding(size_vocab+2, 64, input_length=sentence_size),\n",
    "    Dropout (0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 25, 64)            320128    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              multiple                  49344     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               230500    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 600,073\n",
      "Trainable params: 600,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ms_cnn.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "ms_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8045 samples, validate on 1955 samples\n",
      "Epoch 1/3\n",
      "8045/8045 [==============================] - 6s - loss: 0.6159 - acc: 0.6548 - val_loss: 0.5286 - val_acc: 0.7483\n",
      "Epoch 2/3\n",
      "8045/8045 [==============================] - 6s - loss: 0.4412 - acc: 0.8058 - val_loss: 0.4916 - val_acc: 0.7632\n",
      "Epoch 3/3\n",
      "8045/8045 [==============================] - 6s - loss: 0.3340 - acc: 0.8594 - val_loss: 0.5360 - val_acc: 0.7555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116bc0748>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_cnn.fit(trn_sentences, trn_sentiment, validation_data=(val_sentences, val_sentiment), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_vectors(filename):\n",
    "    vecs, words, wordidx = [], [], {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = line.split(\" \")\n",
    "            vecs.append([float(x) for x in tokens[1:]])\n",
    "            words.append(tokens[0])\n",
    "            wordidx[tokens[0]] = i\n",
    "    return np.array(vecs), np.array(words), wordidx\n",
    "\n",
    "def create_emb(vecs, words, wordidx, vocab_size):\n",
    "    # don't do any casing, should i?\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(0,len(emb)-2):\n",
    "        word = index2word[i]\n",
    "        if word in wordidx and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb[-2] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs, words, wordidx = get_word_vectors(\"../data/glove.twitter.27B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = create_emb(vecs, words, wordidx, size_vocab+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_we = Sequential([\n",
    "    Embedding(size_vocab+2, 50, input_length=sentence_size, \n",
    "              weights=[emb], trainable=False),\n",
    "    Dropout(0.25),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPool1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "conv1_we.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8045 samples, validate on 1955 samples\n",
      "Epoch 1/5\n",
      "8045/8045 [==============================] - 1s - loss: 0.6273 - acc: 0.6525 - val_loss: 0.5510 - val_acc: 0.7269\n",
      "Epoch 2/5\n",
      "8045/8045 [==============================] - 1s - loss: 0.5632 - acc: 0.7192 - val_loss: 0.5465 - val_acc: 0.7243\n",
      "Epoch 3/5\n",
      "8045/8045 [==============================] - 1s - loss: 0.5419 - acc: 0.7339 - val_loss: 0.5180 - val_acc: 0.7540\n",
      "Epoch 4/5\n",
      "8045/8045 [==============================] - 1s - loss: 0.5318 - acc: 0.7392 - val_loss: 0.5139 - val_acc: 0.7494\n",
      "Epoch 5/5\n",
      "8045/8045 [==============================] - 1s - loss: 0.5140 - acc: 0.7534 - val_loss: 0.5086 - val_acc: 0.7545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15606a470>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_we.fit(trn_sentences, trn_sentiment, validation_data=(val_sentences, val_sentiment), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.78242321,  0.74201788]),\n",
       " array([ 0.81948168,  0.69497608]),\n",
       " array([ 0.80052379,  0.71772699]),\n",
       " array([1119,  836]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff_value = 0.5\n",
    "val_sentiment_int = val_sentiment.astype(np.uint8)\n",
    "binary_predictions = [x > cutoff_value for x in conv1.predict(val_sentences)]\n",
    "precision_recall_fscore_support(val_sentiment_int, binary_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_sentence_to_index(sentence):\n",
    "    s = nlp(sentence)\n",
    "    ind_s = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            ind_s.append(word2index[w.text])\n",
    "        except:\n",
    "            ind_s.append(size_vocab)\n",
    "    padded = sequence.pad_sequences([ind_s], maxlen=sentence_size, value=size_vocab+1)[0]\n",
    "    return np.expand_dims(padded, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indx = raw_sentence_to_index(\"I love this movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82185811]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_cnn.predict(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01227814]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx = raw_sentence_to_index(\"The movie seemed good, but in the end, was terrible.\")\n",
    "ms_cnn.predict(indx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
