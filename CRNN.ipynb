{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import torch.distributions as distributions\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of RNN and LSTM using pytorch\n",
    "\n",
    "Source: http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#sphx-glr-intermediate-char-rnn-generation-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"/Users/tfolkman/projects/elder_folkman/content/post/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = \"\"\n",
    "for file in os.listdir(input_directory):\n",
    "    if file == \"Contact.md\":\n",
    "        continue\n",
    "    with open(input_directory + file, \"r\") as f:\n",
    "        meta_counter = 0\n",
    "        for i, line in enumerate(f):\n",
    "            if \"+++\" in line:\n",
    "                meta_counter += 1\n",
    "                continue\n",
    "            elif meta_counter > 1:\n",
    "                clean_line = line.strip().lower()\n",
    "                if len(clean_line) == 0:\n",
    "                    all_lines += \"\\n\"\n",
    "                else:\n",
    "                    all_lines = all_lines + \" \" + clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325408"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim, n_layers=2, bs=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bs = bs\n",
    "        self.word_embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden = self.initHidden()\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeds = self.word_embeddings(input)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(input), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(input), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(self.n_layers, self.bs, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, self.bs, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = set([c.lower() for word in all_lines for c in word])\n",
    "letters_to_index = {l:i for i,l in enumerate(letters)}\n",
    "index_to_letter = {i:l for l, i in letters_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_letters = len(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "batch_size = 1\n",
    "\n",
    "def inputTensor(line):\n",
    "    return torch.LongTensor([letters_to_index[letter] for letter in line[:-1]])\n",
    "\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [letters_to_index[line[li]] for li in range(1, len(line))]\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def randomTrainingPair():\n",
    "    starting_position = np.random.randint(len(all_lines)-(sequence_length+1))\n",
    "    return all_lines[starting_position:starting_position+(sequence_length+1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    line = randomTrainingPair()\n",
    "    input_line_tensor = Variable(inputTensor(line))\n",
    "    target_line_tensor = Variable(targetTensor(line))\n",
    "    return input_line_tensor, target_line_tensor\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "lstm = LSTM(n_letters, 64, n_letters, 64)\n",
    "\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=.001)\n",
    "\n",
    "def train(input_line_tensor, target_line_tensor):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = lstm(input_line_tensor)\n",
    "    loss = criterion(output, target_line_tensor)\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.data.item() / input_line_tensor.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 20000\n",
    "print_every = 100\n",
    "plot_every = 100\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_t, target_t = randomTrainingExample()\n",
    "    output, loss = train(input_t, target_t)\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 5\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(start_letter='\\n'):\n",
    "    output_name = \"\"\n",
    "    input, _ = randomTrainingExample()\n",
    "\n",
    "    for i in range(max_length):\n",
    "        output = lstm(input)\n",
    "        output = output.detach().cpu().numpy()\n",
    "        topi = [np.random.choice(n_letters, p=np.exp(l)) for l in output]\n",
    "        letters = \"\".join([index_to_letter[indx] for indx in topi])\n",
    "        output_name += letters\n",
    "        input = inputTensor(letters)\n",
    "\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
