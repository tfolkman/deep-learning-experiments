{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from comet_ml import Experiment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import models\n",
    "from itertools import count\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dict = {\"LEARNING_RATE\": 1e-5,\n",
    "              \"GAMMA\": 0.99,\n",
    "              \"N_UPDATES\": 100000,\n",
    "              \"GAMES_PER_UPDATE\": 3,\n",
    "              \"SAVE_EVERY_UPATE\": 25,\n",
    "              \"SAVE_FILE\": \"./models/pong_policy_9.state\",\n",
    "              \"RESUME_FILE\": \"./models/pong_policy_8.state\",\n",
    "              \"RESUME\": True,\n",
    "              \"RENDER\": True,\n",
    "              \"SLOW_DOWN\": 0.02,\n",
    "              \"PROJECT_NAME\": \"pong-policy\",\n",
    "              \"ENV\": \"Pong-v0\"}\n",
    "\n",
    "env = gym.make(hyper_dict['ENV'])\n",
    "\n",
    "nnpolicy = models.ConvNet(env.action_space.n).cuda()\n",
    "if hyper_dict[\"RESUME\"]:\n",
    "    nnpolicy.load_state_dict(torch.load(hyper_dict[\"RESUME_FILE\"]))\n",
    "\n",
    "def prepro(I):\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return np.expand_dims(I.astype(np.float), axis=0)\n",
    "\n",
    "\n",
    "def get_outputs(obs):\n",
    "    return nnpolicy(Variable(torch.from_numpy(obs).float().unsqueeze(0).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: We detected that you are running inside a Ipython/Jupyter notebook environment but we cannot save your notebook source code. Please be sure to have installed comet_ml as a notebook server extension by running:\n",
      "jupyter comet_ml enable\n",
      "COMET INFO: old comet version (1.0.9) detected. current: 1.0.13 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET WARNING: Comet.ml support for Ipython Notebook is limited at the moment, automatic monitoring and stdout capturing is deactivated\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/syrios/pong-policy/0c6d2a29788e423ba12a52609533f5a0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"dZm2UV8sODS5eDYysEf8TzKNu\", \n",
    "                        project_name=hyper_dict['PROJECT_NAME'])\n",
    "experiment.log_multiple_params(hyper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    \"\"\"\n",
    "    Discount a list of rewards by GAMMA\n",
    "    Where now the last value in the list is just the reward for that value\n",
    "    Since no more moves happened\n",
    "    And the first gets added upon by the future discounted\n",
    "    Thus have the discounted reward value for each action during the game\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + discount_rate * cumulative_rewards\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(rewards, discount_rate):\n",
    "    \"\"\"\n",
    "    Discount a list of list of rewards and normalize\n",
    "    \"\"\"\n",
    "    all_discounted_rewards = [discount_rewards(reward, discount_rate) \n",
    "                              for reward in rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/(reward_std + np.finfo(np.float32).eps)\n",
    "            for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "def update_model(all_rewards, all_gradients, discount_rate):\n",
    "    \"\"\"\n",
    "    Get discounted and normalized rewards for each move from each game\n",
    "    Loop over every game and take the sum of r * -1 * gradient to understand\n",
    "    How each gradient should be updated based on that game\n",
    "    Then average over your batch\n",
    "    \"\"\"\n",
    "    # get discounted rewards -> discount rewards which happened \n",
    "    # later more and normalize\n",
    "    loss = []\n",
    "    all_rewards = discount_and_normalize_rewards(all_rewards,discount_rate)\n",
    "    # for every game played, sum up the total losses where you take the loss \n",
    "    # (*-1 since log_prob is ASCENT)\n",
    "    # and multiply by the discounted, normalized reward. Thus, actions that lead \n",
    "    # to good rewards are applied\n",
    "    # And actions that lead to be rewards (negative) are inversely applied.\n",
    "    # Thus, that sum tells you how much to update based on that game\n",
    "    for step in range(len(all_rewards)):\n",
    "        r = torch.Tensor(all_rewards[step]).cuda()\n",
    "        step_loss = []\n",
    "        for value in range(len(all_rewards[step])):\n",
    "            step_loss.append(r[value] * all_gradients[step][value] * -1)\n",
    "        loss.append(sum(step_loss))\n",
    "    loss = torch.cat(loss)\n",
    "    optimizer.zero_grad()\n",
    "    # Take the mean of all games losses to help smooth out the learning (mini-batch)\n",
    "    policy_loss = loss.mean()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "def getGradients(n_iters, n_games_per_gradient, discount_rate):\n",
    "    \"\"\"\n",
    "    For each n_iters:\n",
    "        Play n_games_per_gradients and use the policy to probabilistically\n",
    "        Move based output probabilities\n",
    "        Save gradients and rewards\n",
    "        After playing the batch of games, update gradients\n",
    "        \n",
    "    \"\"\"\n",
    "    running_reward = None\n",
    "    reward_sum = 0\n",
    "    for iteration in range(n_iters):\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_gradient):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "\n",
    "            obs = env.reset()\n",
    "            prev_x = None\n",
    "            for step in count():\n",
    "                if hyper_dict['RENDER']:\n",
    "                    env.render()\n",
    "                # get the predicted action probabilities from our nn policy\n",
    "                cur_x = prepro(obs)\n",
    "                x = cur_x - prev_x if prev_x is not None else np.zeros((1,80,80))\n",
    "                prev_x = cur_x\n",
    "                outputs = get_outputs(x)\n",
    "                #select an action with these probabilities\n",
    "                categorical_distribution = Categorical(outputs)\n",
    "                seleted_action = categorical_distribution.sample()\n",
    "                #save the loss function\n",
    "                current_gradients.append(\n",
    "                    categorical_distribution.log_prob(seleted_action))\n",
    "                #apply the action\n",
    "                action = seleted_action.item()\n",
    "                #save the reward\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "                reward_sum += reward\n",
    "                current_rewards.append(reward)\n",
    "                if hyper_dict[\"SLOW_DOWN\"] is not None:\n",
    "                    time.sleep(hyper_dict[\"SLOW_DOWN\"]) \n",
    "                if done:\n",
    "                    running_reward = reward_sum if running_reward is None \\\n",
    "                        else running_reward * 0.99 + reward_sum * 0.01\n",
    "                    experiment.log_metric(\"reward sum\", reward_sum, \n",
    "                                          step=game + iteration * n_games_per_gradient)\n",
    "                    reward_sum = 0\n",
    "                    experiment.log_metric(\"reward mean\", \n",
    "                                          running_reward,\n",
    "                                          step=game + iteration * n_games_per_gradient)\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "        # apply saved loss functions\n",
    "        update_model(all_rewards, all_gradients, discount_rate)\n",
    "        if iteration % hyper_dict[\"SAVE_EVERY_UPATE\"] == 0:\n",
    "            print(\"Iteration: {}\".format(iteration))\n",
    "            print(\"Saving Model...\")\n",
    "            torch.save(nnpolicy.state_dict(), hyper_dict[\"SAVE_FILE\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(nnpolicy.parameters(), lr=hyper_dict['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGradients(n_iters = hyper_dict['N_UPDATES'],\n",
    "                       n_games_per_gradient = hyper_dict['GAMES_PER_UPDATE'],\n",
    "                       discount_rate = hyper_dict['GAMMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
