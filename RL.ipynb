{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Policy ->algo used to determine actions; neural net take in obs and select an action\n",
    "* Policy Gradients -> basically using gradient descent to tweak parameters to maximize rewards\n",
    "* Pick the highest predicted action with some probability to allow exploitation and exploration\n",
    "* Credit assignment problem -> when an agent gets a reward, hard to know which action should get assignment. To solve, usually evaluate an action based on sum of all rewards which came after it - discounted - discounting later actions more. Discount rates are usually 0.95 or 0.99. 0.95 => 13 steps into future 1/2 as important. 69 steps with 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Net Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNPolicy(nn.Module):\n",
    "    def __init__(self, n_inputs=4, n_hidden=4, n_outputs=2):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.hidden1 = nn.Linear(n_inputs, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = F.elu(self.hidden1(x))\n",
    "        output = self.out(output)\n",
    "        return F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpolicy = NNPolicy()\n",
    "optimizer = optim.Adam(nnpolicy.parameters(), lr=1e-2)\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + discount_rate * cumulative_rewards\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(reward, discount_rate) for reward in rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/(reward_std + np.finfo(np.float32).eps)\n",
    "            for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "def update_model(all_rewards, all_gradients, discount_rate):\n",
    "    # get discounted rewards -> discount rewards which happened later more and normalize\n",
    "    loss = []\n",
    "    all_rewards = discount_and_normalize_rewards(all_rewards,discount_rate)\n",
    "    # for every game played, sum up the total losses where you take the loss (*-1 since log_prob is ASCENT)\n",
    "    # and multiply by the discounted, normalized reward. Thus, actions that lead to good rewards are applied\n",
    "    # And actions that lead to be rewards (negative) are inversely applied.\n",
    "    # Thus, that sum tells you how much to update based on that game\n",
    "    for step in range(len(all_rewards)):\n",
    "        r = torch.Tensor(all_rewards[step])\n",
    "        step_loss = []\n",
    "        for value in range(len(all_rewards[step])):\n",
    "            step_loss.append(r[value] * all_gradients[step][value] * -1)\n",
    "        loss.append(sum(step_loss))\n",
    "    loss = torch.cat(loss)\n",
    "    optimizer.zero_grad()\n",
    "    # Take the mean of all games losses to help smooth out the learning (mini-batch)\n",
    "    policy_loss = loss.mean()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def getGradients(n_iters = 100, n_max_steps=1000, n_games_per_gradient=10, discount_rate=0.95,\n",
    "                render_rate=100, log_rate=100):\n",
    "    n_steps = []\n",
    "    for iteration in range(n_iters):\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        if (iteration + 1) % log_rate ==0:\n",
    "            print(iteration+1)\n",
    "            print(\"Average Steps: {}\".format(np.mean(n_steps)))\n",
    "            print(\"Last Steps: {}\".format(n_steps[-1]))\n",
    "            print(\"Max Steps: {}\".format(np.max(n_steps)))\n",
    "            print(\"*****\")\n",
    "        for game in range(n_games_per_gradient):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            n_steps.append(n_max_steps)\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                # get the predicted action probabilities from our nn policy\n",
    "                outputs = nnpolicy(Variable(torch.from_numpy(obs).float().unsqueeze(0)))\n",
    "                #select an action with these probabilities\n",
    "                categorical_distribution = Categorical(outputs)\n",
    "                seleted_action = categorical_distribution.sample()\n",
    "                #save the loss function\n",
    "                current_gradients.append(categorical_distribution.log_prob(seleted_action))\n",
    "                #apply the action\n",
    "                action = seleted_action.data[0]\n",
    "                #save the reward\n",
    "                print(action)\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "                if (iteration + 1) % render_rate ==0:\n",
    "                    env.render()\n",
    "                current_rewards.append(reward)\n",
    "                if done:\n",
    "                    n_steps[-1] = step\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "        # apply saved loss functions\n",
    "        update_model(all_rewards, all_gradients, discount_rate)\n",
    "    print(n_steps[:10])\n",
    "    print(n_steps[-10:])\n",
    "    return n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor(0) (<class 'torch.Tensor'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ce5837bab536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-c21635220fcf>\u001b[0m in \u001b[0;36mgetGradients\u001b[0;34m(n_iters, n_max_steps, n_games_per_gradient, discount_rate, render_rate, log_rate)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m#save the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrender_rate\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor(0) (<class 'torch.Tensor'>) invalid"
     ]
    }
   ],
   "source": [
    "n = 150\n",
    "n_steps = getGradients(n_iters = n, render_rate=n, log_rate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
