{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named torch.autograd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e17ac0952c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named torch.autograd"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.modules.padding as padding\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from comet_ml import Experiment\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(api_key=\"7bEW5h9UoEOpQQyNLpt36lY66\", project_name=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mspacman_color = np.array([210, 164, 74]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    #crop and downsize. ::2 takes everyother, 1:176:2 takes every other in that range\n",
    "    img = obs[1:176:2, ::2]\n",
    "    # take mean over color channels to get greyscale\n",
    "    img = img.mean(axis=2)\n",
    "    # improve contrast of ms pacman\n",
    "    img[img==mspacman_color] = 0\n",
    "    # normalize data from -1 to 1\n",
    "    img = (img-128) / 128 - 1\n",
    "    return img.reshape(88, 80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "Store transitions that the agent observes so can be re-used later. By sampling from this randomly improves stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                       ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return np.random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Our network takes in an image and tries to predict the quality\n",
    "    of taking each of our 9 actions given that state (the image)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "num_episodes = 50\n",
    "N_ACTIONS = 9\n",
    "\n",
    "experiment.log_metric(\"batch size\", BATCH_SIZE)\n",
    "experiment.log_metric(\"gamma\", GAMMA)\n",
    "experiment.log_metric(\"eps start\", EPS_START)\n",
    "experiment.log_metric(\"eps end\", EPS_END)\n",
    "experiment.log_metric(\"eps decay\", EPS_DECAY)\n",
    "experiment.log_metric(\"target update\", TARGET_UPDATE)\n",
    "experiment.log_metric(\"num episodes\", num_episodes)\n",
    "\n",
    "\n",
    "# the policy network is used to play the game - aka actor\n",
    "policy_net = DQN(N_ACTIONS).cuda()\n",
    "# the target net is used to predict Q values for next action\n",
    "# we need 2 otherwise we would be using the same network\n",
    "# in the actual and predicted values of our loss function\n",
    "target_net = DQN(N_ACTIONS).cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "# sets training to false \n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    # gen random number\n",
    "    sample = random.random()\n",
    "    # get threshold which decays from start to end \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    # if exceed, pick best\n",
    "    if sample > eps_threshold:\n",
    "        # No gradients b/c not learning, just getting best one\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1,1)\n",
    "    # else, random\n",
    "    else:\n",
    "        return torch.LongTensor([[np.random.randint(N_ACTIONS)]]).cuda()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # transpose the batch...\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # compute mask of transitions which didn't lead to ending game\n",
    "    non_final_mask = torch.IntTensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state))).cuda()\n",
    "    # the next states for non final states\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                      if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # calculate the Q value of taking the state, action pairs which were taken\n",
    "    # the gather basically takes the Q value for the action choosen\n",
    "    # So if my input choose action 3 for the given state, that is what I would gather.\n",
    "    # Basically, what is the Q value for what actually happened\n",
    "    # Q value being the total expected value from taking an action given a state.\n",
    "    # These are basically our predictions for learning\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE).cuda()\n",
    "    # get the best actions for the next states\n",
    "    # detach is for speed so don't calc gradients\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # the expected value of the Q(s,a) given from the policy net is the\n",
    "    # reward given plus the discounted value of the Q value from taking the best\n",
    "    # action at the next step\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, \n",
    "                            expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the policy net to become better and predicting Q values\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # clip gradients to prevent exploding gradient\n",
    "    # clipped between -1 and 1\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    state = preprocess_observation(env.reset())\n",
    "    losses = []\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        if not done:\n",
    "            next_state = preprocess_observation(next_state)\n",
    "        else:\n",
    "            next_state = None\n",
    "        reward = torch.tensor([reward]).cuda()\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        losses.append(optimize_model())\n",
    "        if done:\n",
    "            experiment.log_metric(\"duration\", t+1, step=i_episode)\n",
    "            experiment.log_metric(\"avg loss\", mean(losses), step=i_episode)\n",
    "            break\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"./models/dqn_polich.state\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
