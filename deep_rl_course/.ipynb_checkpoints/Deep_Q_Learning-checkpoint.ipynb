{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import PIL\n",
    "from collections import namedtuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "action_size = 3              # 3 possible actions: left, right, shoot\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 1000        # Total episodes for training\n",
    "max_steps = 300             # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 7000              # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95              # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False\n",
    "\n",
    "TrainingParameters = namedtuple('TrainingParameters', ['total_episodes', 'max_steps', 'batch_size',\n",
    "                                                      'explore_start', 'explore_stop', 'decay_rate', 'gamma',\n",
    "                                                      'device'])\n",
    "train_params = TrainingParameters(total_episodes, max_steps, batch_size, explore_start,\n",
    "                                 explore_stop, decay_rate, gamma, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Class that handles the replay buffer and takes in the raw numpy and transforms it as necessary\n",
    "    Transforms:\n",
    "        Crop out the unnecessary parts of image\n",
    "        Normalize to 0-1\n",
    "        Resize to 84x84\n",
    "    Also handles stacking the frames together\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, stack_size=4, memory_size = 1000000):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize((84,84)), transforms.ToTensor()])\n",
    "        self.stacked_frames  =  deque([torch.zeros((84,84)) for i in range(stack_size)], maxlen=4) \n",
    "        self.device = device\n",
    "        self.stack_size = stack_size\n",
    "        self.MemoryItem = namedtuple('MemoryItem', ['state', 'action', 'reward', 'next_state', 'not_done'])\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def _transform(self, x):\n",
    "        img = PIL.Image.fromarray(x)\n",
    "        img_cropped = transforms.functional.crop(img,30,30,80,100)\n",
    "        return self.img_transforms(img_cropped).to(self.device)\n",
    "    \n",
    "    def _combine_memories(self, memories):\n",
    "        states, actions, rewards, next_states, not_dones = zip(*memories)\n",
    "        return self.MemoryItem(torch.cat(states).to(self.device),\n",
    "                               torch.LongTensor(actions).to(self.device), \n",
    "                               torch.FloatTensor(rewards).to(self.device),\n",
    "                               torch.cat(next_states).to(self.device),\n",
    "                               torch.FloatTensor(not_dones).to(self.device))\n",
    "    \n",
    "    def stack_frames(self, state, is_new_episode):\n",
    "        # Preprocess frame\n",
    "        frame = self._transform(state)\n",
    "\n",
    "        if is_new_episode:\n",
    "            # Clear our stacked_frames\n",
    "            self.stacked_frames = deque([torch.zeros((84,84)) for i in range(self.stack_size)], maxlen=4)\n",
    "\n",
    "            # Because we're in a new episode, copy the same frame 4x\n",
    "            self.stacked_frames.append(frame)\n",
    "            self.stacked_frames.append(frame)\n",
    "            self.stacked_frames.append(frame)\n",
    "            self.stacked_frames.append(frame)\n",
    "\n",
    "            # Stack the frames\n",
    "            stacked_state = torch.stack(list(self.stacked_frames), 1)\n",
    "\n",
    "        else:\n",
    "            # Append frame to deque, automatically removes the oldest frame\n",
    "            self.stacked_frames.append(frame)\n",
    "\n",
    "            # Build the stacked state (first dimension specifies different frames)\n",
    "            stacked_state = torch.stack(list(self.stacked_frames), 1)\n",
    "\n",
    "        return stacked_state\n",
    "    \n",
    "    def push(self, stacked_state, action, reward, stacked_next_state, not_done):\n",
    "        \"\"\"\n",
    "        seems to be working\n",
    "        \"\"\"\n",
    "        self.memory.append(self.MemoryItem(stacked_state, action, reward, stacked_next_state, not_done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        pretty sure works\n",
    "        \"\"\"\n",
    "        indxs = np.random.choice(range(len(self.memory)), batch_size, replace=False)\n",
    "        return self._combine_memories([self.memory[i] for i in indxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        \n",
    "class DeepQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(8*8*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = nn.functional.elu(self.bn1(self.conv1(x)))\n",
    "        out_2 = nn.functional.elu(self.bn2(self.conv2(out_1)))\n",
    "        out_3 = nn.functional.elu(self.bn3(self.conv3(out_2)))\n",
    "        out_4 = nn.functional.elu(self.fc1(out_3.view(x.shape[0], -1)))\n",
    "        out_5 = self.fc2(out_4)\n",
    "        return out_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Deep Q Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_new_game(game):\n",
    "    \"\"\"\n",
    "    seems to work\n",
    "    \"\"\"\n",
    "    game.new_episode()\n",
    "    game_start = True\n",
    "    state = game.get_state().screen_buffer\n",
    "    return game, state, game_start\n",
    "\n",
    "def take_action(game, action):\n",
    "    \"\"\"\n",
    "    seems to work\n",
    "    \"\"\"\n",
    "    return game.make_action(action), game.is_episode_finished()\n",
    "\n",
    "def handle_done(game, stacked_state, action, reward, game_start, memory):\n",
    "    stacked_next_state = torch.zeros_like(stacked_state)\n",
    "    memory.push(stacked_state, action, reward, stacked_next_state, False)\n",
    "    \n",
    "def handle_not_done(game, stacked_state, action, reward, game_start, memory):\n",
    "    \"\"\"\n",
    "    seems to be working\n",
    "    \"\"\"\n",
    "    next_state = game.get_state().screen_buffer\n",
    "    stacked_next_state = memory.stack_frames(next_state, game_start)\n",
    "    memory.push(stacked_state, action, reward, stacked_next_state, True)\n",
    "    return next_state, False\n",
    "\n",
    "def initalize_memory(pretrain_length, game, possible_actions, memory):\n",
    "\n",
    "    game, state, game_start = start_new_game(game)\n",
    "    \n",
    "    for i in range(pretrain_length):\n",
    "\n",
    "        # Random action\n",
    "        action = random.choice(possible_actions)\n",
    "        reward, done = take_action(game, action)\n",
    "        stacked_state = memory.stack_frames(state, game_start)\n",
    "\n",
    "        # If we're dead\n",
    "        if done:\n",
    "            handle_done(game, stacked_state, action, reward, game_start, \n",
    "                        memory)\n",
    "            game, state, game_start = start_new_game(game)\n",
    "\n",
    "        else:\n",
    "            state, game_start = handle_not_done(game, stacked_state, \n",
    "                                                action, \n",
    "                                                reward, game_start,\n",
    "                                                memory)\n",
    "\n",
    "def epsilon_greedy_move(game, model, state, possible_actions, train_params, steps_done):\n",
    "    \"\"\"\n",
    "    seems to be working\n",
    "    \"\"\"\n",
    "    eps_threshold = train_params.explore_stop + (train_params.explore_start - train_params.explore_stop) \\\n",
    "        * math.exp(-1. * steps_done / train_params.decay_rate)\n",
    "    if np.random.rand() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            action = possible_actions[int(torch.argmax(model(state)))]\n",
    "    else:\n",
    "        action = random.choice(possible_actions)\n",
    "    reward, done = take_action(game, action)\n",
    "    return reward, action, done, eps_threshold\n",
    "    \n",
    "def update_model(model, memory, optim, train_params):\n",
    "    \"\"\"\n",
    "    target is being calculated correctly\n",
    "    selecting max q correctly\n",
    "    correct action index being selected\n",
    "    selecting correct q value from model (predicted)\n",
    "    \"\"\"\n",
    "    memory_sample = memory.sample(train_params.batch_size)\n",
    "    max_q_next_state, _ = torch.max(model(memory_sample.next_state).detach(),1)\n",
    "    target = memory_sample.reward + (train_params.gamma * max_q_next_state * memory_sample.not_done)\n",
    "    action_indexes = torch.argmax(memory_sample.action,1).unsqueeze(1)\n",
    "    action_indexes = action_indexes.to(train_params.device)\n",
    "    predicted = model(memory_sample.state).gather(1, action_indexes).squeeze(1)\n",
    "    loss = nn.functional.mse_loss(predicted, target)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(game, model, memory, possible_actions, train_params,\n",
    "         print_every=1000):\n",
    "    \"\"\"\n",
    "    model params changing\n",
    "    \"\"\"\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    total_steps = 0\n",
    "    losses = []\n",
    "    game.init()\n",
    "    for episode in range(train_params.total_episodes):\n",
    "        game, state, game_start = start_new_game(game)\n",
    "        for step in range(train_params.max_steps):\n",
    "            \n",
    "            stacked_state = memory.stack_frames(state, game_start)\n",
    "\n",
    "            reward, action, done, eps_threshold = epsilon_greedy_move(game, model, stacked_state, \n",
    "                                                              possible_actions,\n",
    "                                                              train_params, total_steps)\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                handle_done(game, stacked_state, action, reward, game_start, memory)\n",
    "                break\n",
    "            else:\n",
    "                state, game_start = handle_not_done(game, stacked_state, action, reward, game_start, memory)  \n",
    "                \n",
    "            loss = update_model(model, memory, optim, train_params)\n",
    "            losses.append(loss)\n",
    "\n",
    "            if total_steps % print_every == 0:\n",
    "                print(\"Loss: {}\".format(loss))\n",
    "                print(\"Explore Prob: {}\".format(eps_threshold))\n",
    "                print(\"Epoch: {}\".format(episode))\n",
    "                \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# initalize all the stuff we need\n",
    "\n",
    "memory = ReplayMemory(device)\n",
    "model = DeepQ().to(device)\n",
    "model.apply(init_weights)\n",
    "game, possible_actions = create_environment()\n",
    "game.init()\n",
    "initalize_memory(pretrain_length, game, possible_actions, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 84, 84])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.sample(2).state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2061837911605835\n",
      "Explore Prob: 0.8683317308132975\n",
      "Epoch: 5\n",
      "Loss: 2.0623860359191895\n",
      "Explore Prob: 0.7540688080963694\n",
      "Epoch: 9\n",
      "Loss: 0.8283056616783142\n",
      "Explore Prob: 0.6550168056322018\n",
      "Epoch: 16\n",
      "Loss: 2.688368320465088\n",
      "Explore Prob: 0.5691508137700142\n",
      "Epoch: 21\n",
      "Loss: 162.6719970703125\n",
      "Explore Prob: 0.4947154830845548\n",
      "Epoch: 26\n",
      "Loss: 1.8158774375915527\n",
      "Explore Prob: 0.4301891399527336\n",
      "Epoch: 30\n",
      "Loss: 1.2984395027160645\n",
      "Explore Prob: 0.3742526791400608\n",
      "Epoch: 35\n",
      "Loss: 0.6760739684104919\n",
      "Explore Prob: 0.32576259747131275\n",
      "Epoch: 42\n",
      "Loss: 1.5486111640930176\n",
      "Explore Prob: 0.2837276173155936\n",
      "Epoch: 49\n",
      "Loss: 163.4134063720703\n",
      "Explore Prob: 0.24728842200216322\n",
      "Epoch: 54\n",
      "Loss: 159.51052856445312\n",
      "Explore Prob: 0.21570008890027\n",
      "Epoch: 61\n",
      "Loss: 1.162790298461914\n",
      "Explore Prob: 0.18831686104429174\n",
      "Epoch: 68\n",
      "Loss: 161.9590606689453\n",
      "Explore Prob: 0.1645789459921206\n",
      "Epoch: 73\n",
      "Loss: 1.6024041175842285\n",
      "Explore Prob: 0.1440010720472463\n",
      "Epoch: 78\n",
      "Loss: 6.265480995178223\n",
      "Explore Prob: 0.1261625679005896\n",
      "Epoch: 87\n",
      "Loss: 2.862903118133545\n",
      "Explore Prob: 0.11069876289125098\n",
      "Epoch: 92\n",
      "Loss: 0.6863152980804443\n",
      "Explore Prob: 0.09729353208260919\n",
      "Epoch: 98\n",
      "Loss: 4.642895698547363\n",
      "Explore Prob: 0.08567283375354735\n",
      "Epoch: 102\n",
      "Loss: 163.6291046142578\n",
      "Explore Prob: 0.07559910719241979\n",
      "Epoch: 105\n",
      "Loss: 0.40566110610961914\n",
      "Explore Prob: 0.06686641626845188\n",
      "Epoch: 110\n",
      "Loss: 3.825211524963379\n",
      "Explore Prob: 0.05929623950111513\n",
      "Epoch: 116\n",
      "Loss: 141.4574432373047\n",
      "Explore Prob: 0.05273382056430863\n",
      "Epoch: 121\n",
      "Loss: 1.517184853553772\n",
      "Explore Prob: 0.042113495800431645\n",
      "Epoch: 135\n",
      "Loss: 1.8161298036575317\n",
      "Explore Prob: 0.03783847979311446\n",
      "Epoch: 140\n",
      "Loss: 99.5329818725586\n",
      "Explore Prob: 0.034132562895292935\n",
      "Epoch: 149\n",
      "Loss: 1.8592628240585327\n",
      "Explore Prob: 0.030919985438260696\n",
      "Epoch: 158\n",
      "Loss: 2.4202675819396973\n",
      "Explore Prob: 0.02813507303952381\n",
      "Epoch: 164\n",
      "Loss: 2.732907772064209\n",
      "Explore Prob: 0.02572089402831855\n",
      "Epoch: 170\n",
      "Loss: 62.53355407714844\n",
      "Explore Prob: 0.023628095597463962\n",
      "Epoch: 179\n",
      "Loss: 7.884657859802246\n",
      "Explore Prob: 0.02181389488912425\n",
      "Epoch: 183\n",
      "Loss: 39.49721145629883\n",
      "Explore Prob: 0.02024120438935343\n",
      "Epoch: 192\n",
      "Loss: 107.370361328125\n",
      "Explore Prob: 0.01887787375195505\n",
      "Epoch: 200\n",
      "Loss: 5.876570701599121\n",
      "Explore Prob: 0.017696032552342054\n",
      "Epoch: 218\n",
      "Loss: 105.442626953125\n",
      "Explore Prob: 0.016671520535383313\n",
      "Epoch: 225\n",
      "Loss: 13.685718536376953\n",
      "Explore Prob: 0.015783393709853294\n",
      "Epoch: 233\n",
      "Loss: 8.186318397521973\n",
      "Explore Prob: 0.015013496192626036\n",
      "Epoch: 241\n",
      "Loss: 8.196929931640625\n",
      "Explore Prob: 0.014346089049869188\n",
      "Epoch: 245\n",
      "Loss: 18.590566635131836\n",
      "Explore Prob: 0.013767528547677862\n",
      "Epoch: 249\n",
      "Loss: 36.765220642089844\n",
      "Explore Prob: 0.01326598723465984\n",
      "Epoch: 253\n",
      "Loss: 21.766799926757812\n",
      "Explore Prob: 0.012831212154592824\n",
      "Epoch: 258\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9bdc558f8737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0d7e792e5dc2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(game, model, memory, possible_actions, train_params, print_every)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_not_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0d7e792e5dc2>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(model, memory, optim, train_params)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "losses = train(game, model, memory, possible_actions, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
