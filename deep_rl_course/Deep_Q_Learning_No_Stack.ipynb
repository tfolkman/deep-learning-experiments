{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import PIL\n",
    "from collections import namedtuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False\n",
    "render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(api_key=\"dZm2UV8sODS5eDYysEf8TzKNu\", \n",
    "                        project_name=\"DQN_DOOM\", disabled=not train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "action_size = 3              # 3 possible actions: left, right, shoot\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 2000        # Total episodes for training\n",
    "max_steps = 100             # Max possible steps in an episode\n",
    "batch_size = 128             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0           # exploration probability at start\n",
    "explore_stop = 0.01             # minimum exploration probability \n",
    "decay_rate = 20000             # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.99             # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000             # Number of experiences the Memory can keep\n",
    "\n",
    "\n",
    "TrainingParameters = namedtuple('TrainingParameters', ['total_episodes', 'max_steps', 'batch_size',\n",
    "                                                      'explore_start', 'explore_stop', 'decay_rate', 'gamma',\n",
    "                                                      'device'])\n",
    "train_params = TrainingParameters(total_episodes, max_steps, batch_size, explore_start,\n",
    "                                 explore_stop, decay_rate, gamma, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(visible=True):\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    if visible:\n",
    "        game.set_window_visible(True)\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Class that handles the replay buffer and takes in the raw numpy and transforms it as necessary\n",
    "    Transforms:\n",
    "        Crop out the unnecessary parts of image\n",
    "        Normalize to 0-1\n",
    "        Resize to 84x84\n",
    "    Also handles stacking the frames together\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, stack_size=4, memory_size = 1000000):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize((84,84)), transforms.ToTensor()])\n",
    "        self.device = device\n",
    "        self.MemoryItem = namedtuple('MemoryItem', ['state', 'action', 'reward', 'next_state', 'not_done'])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def _combine_memories(self, memories):\n",
    "        states, actions, rewards, next_states, not_dones = zip(*memories)\n",
    "        return self.MemoryItem(torch.cat(states).to(self.device),\n",
    "                               torch.LongTensor(actions).to(self.device), \n",
    "                               torch.FloatTensor(rewards).to(self.device),\n",
    "                               torch.cat(next_states).to(self.device),\n",
    "                               torch.FloatTensor(not_dones).to(self.device))\n",
    "    \n",
    "    def transform(self, x):\n",
    "        img = PIL.Image.fromarray(x)\n",
    "        img_cropped = transforms.functional.crop(img,30,30,80,100)\n",
    "        return self.img_transforms(img_cropped).to(self.device).unsqueeze(0)\n",
    "    \n",
    "    def push(self, state_trans, action, reward, next_state_trans, not_done):\n",
    "        self.memory.append(self.MemoryItem(state_trans, action, reward, next_state_trans, not_done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indxs = np.random.choice(range(len(self.memory)), batch_size, replace=False)\n",
    "        return self._combine_memories([self.memory[i] for i in indxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        \n",
    "class DeepQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 8, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(8*8*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = nn.functional.elu(self.bn1(self.conv1(x)))\n",
    "        out_2 = nn.functional.elu(self.bn2(self.conv2(out_1)))\n",
    "        out_3 = nn.functional.elu(self.bn3(self.conv3(out_2)))\n",
    "        out_4 = nn.functional.elu(self.fc1(out_3.view(x.shape[0], -1)))\n",
    "        out_5 = self.fc2(out_4)\n",
    "        return out_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Deep Q Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_new_game(game):\n",
    "    \"\"\"\n",
    "    seems to work\n",
    "    \"\"\"\n",
    "    game.new_episode()\n",
    "    game_start = True\n",
    "    state = game.get_state().screen_buffer\n",
    "    return game, state, game_start\n",
    "\n",
    "def take_action(game, action):\n",
    "    \"\"\"\n",
    "    seems to work\n",
    "    \"\"\"\n",
    "    return game.make_action(action), game.is_episode_finished()\n",
    "\n",
    "def handle_done(game, state_trans, action, reward, game_start, memory):\n",
    "    next_state = torch.zeros_like(state_trans)\n",
    "    memory.push(state_trans, action, reward, next_state, False)\n",
    "    \n",
    "def handle_not_done(game, state_trans, action, reward, game_start, memory):\n",
    "    \"\"\"\n",
    "    seems to be working\n",
    "    \"\"\"\n",
    "    next_state = game.get_state().screen_buffer\n",
    "    next_state_trans = memory.transform(next_state)\n",
    "    memory.push(state_trans, action, reward, next_state_trans, True)\n",
    "    return next_state, False\n",
    "\n",
    "def initalize_memory(pretrain_length, game, possible_actions, memory):\n",
    "\n",
    "    game, state, game_start = start_new_game(game)\n",
    "    \n",
    "    for i in range(pretrain_length):\n",
    "\n",
    "        state_trans = memory.transform(state)\n",
    "        \n",
    "        # Random action\n",
    "        action = random.choice(possible_actions)\n",
    "        reward, done = take_action(game, action)\n",
    "\n",
    "        # If we're dead\n",
    "        if done:\n",
    "            handle_done(game, state_trans, action, reward, game_start, memory)\n",
    "            game, state, game_start = start_new_game(game)\n",
    "\n",
    "        else:\n",
    "            state, game_start = handle_not_done(game, state_trans, action, reward, game_start, memory)\n",
    "\n",
    "def epsilon_greedy_move(game, model, state, possible_actions, train_params, steps_done):\n",
    "    \"\"\"\n",
    "    seems to be working\n",
    "    \"\"\"\n",
    "    eps_threshold = train_params.explore_stop + (train_params.explore_start - train_params.explore_stop) \\\n",
    "        * math.exp(-1. * steps_done / train_params.decay_rate)\n",
    "    if np.random.rand() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            action = possible_actions[int(torch.argmax(model(state)))]\n",
    "    else:\n",
    "        action = random.choice(possible_actions)\n",
    "    reward, done = take_action(game, action)\n",
    "    return reward, action, done, eps_threshold\n",
    "    \n",
    "def update_model(model, memory, optim, train_params):\n",
    "    \"\"\"\n",
    "    target is being calculated correctly\n",
    "    selecting max q correctly\n",
    "    correct action index being selected\n",
    "    selecting correct q value from model (predicted)\n",
    "    \"\"\"\n",
    "    memory_sample = memory.sample(train_params.batch_size)\n",
    "    max_q_next_state, _ = torch.max(model(memory_sample.next_state).detach(),1)\n",
    "    target = memory_sample.reward + (train_params.gamma * max_q_next_state * memory_sample.not_done)\n",
    "    action_indexes = torch.argmax(memory_sample.action,1).unsqueeze(1)\n",
    "    action_indexes = action_indexes.to(train_params.device)\n",
    "    predicted = model(memory_sample.state).gather(1, action_indexes).squeeze(1)\n",
    "    loss = nn.functional.smooth_l1_loss(predicted, target)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(game, model, memory, possible_actions, train_params,\n",
    "         print_every=1000, save_file=\"../models/doom_dqn.state\"):\n",
    "    \"\"\"\n",
    "    model params changing\n",
    "    \"\"\"\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    total_steps = 0\n",
    "    game.init()\n",
    "    for episode in range(train_params.total_episodes):\n",
    "        game, state, game_start = start_new_game(game)\n",
    "        for step in range(train_params.max_steps):\n",
    "            \n",
    "            state_trans = memory.transform(state)\n",
    "\n",
    "            reward, action, done, eps_threshold = epsilon_greedy_move(game, model, state_trans, \n",
    "                                                              possible_actions,\n",
    "                                                              train_params, total_steps)\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                handle_done(game, state_trans, action, reward, game_start, memory)\n",
    "                break\n",
    "            else:\n",
    "                state, game_start = handle_not_done(game, state_trans, action, reward, game_start, memory)  \n",
    "                \n",
    "            loss = update_model(model, memory, optim, train_params)\n",
    "            experiment.log_metrics({\"reward\": reward, \"loss\": loss}, step=total_steps)\n",
    "\n",
    "            if total_steps % print_every == 0:\n",
    "                print(\"Loss: {}\".format(loss))\n",
    "                print(\"Explore Prob: {}\".format(eps_threshold))\n",
    "                print(\"Epoch: {}\".format(episode))\n",
    "                print(\"Saving...\")\n",
    "                torch.save(model.state_dict(), save_file)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    memory = ReplayMemory(device)\n",
    "    model = DeepQ().to(device)\n",
    "    model.apply(init_weights)\n",
    "    game, possible_actions = create_environment(render)\n",
    "    game.init()\n",
    "    initalize_memory(pretrain_length, game, possible_actions, memory)\n",
    "    train(game, model, memory, possible_actions, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(game, possible_actions, model):\n",
    "    game.init()\n",
    "    memory = ReplayMemory(device)\n",
    "    for i in range(5): \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = memory.transform(game.get_state().screen_buffer)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            predictions = model(state)\n",
    "            action = possible_actions[int(torch.argmax(predictions))]      \n",
    "            game.make_action(action)\n",
    "            if game.is_episode_finished():\n",
    "                break\n",
    "            score = game.get_total_reward()\n",
    "            state = memory.transform(game.get_state().screen_buffer)\n",
    "            time.sleep(.05)\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  94.0\n",
      "Score:  59.0\n",
      "Score:  71.0\n",
      "Score:  95.0\n",
      "Score:  53.0\n"
     ]
    }
   ],
   "source": [
    "if not train_model:\n",
    "    game, possible_actions = create_environment(render)\n",
    "    model = DeepQ().to(device)\n",
    "    model.load_state_dict(torch.load(\"../models/doom_dqn.state\"))\n",
    "    play_game(game, possible_actions, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
