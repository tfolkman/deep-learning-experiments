{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import PIL\n",
    "from collections import namedtuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "action_size = 3              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100             # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.05            # minimum exploration probability \n",
    "decay_rate = 1024              # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False\n",
    "\n",
    "TrainingParameters = namedtuple('TrainingParameters', ['learning_rate', 'total_episodes', 'max_steps', 'batch_size',\n",
    "                                                      'explore_start', 'explore_stop', 'decay_rate', 'gamma',\n",
    "                                                      'device'])\n",
    "train_params = TrainingParameters(learning_rate, total_episodes, max_steps, batch_size, explore_start,\n",
    "                                 explore_stop, decay_rate, gamma, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Class that handles the replay buffer and takes in the raw numpy and transforms it as necessary\n",
    "    Transforms:\n",
    "        Crop out the unnecessary parts of image\n",
    "        Normalize to 0-1\n",
    "        Resize to 84x84\n",
    "    Also handles stacking the frames together\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, stack_size=4, memory_size = 1000000):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize((84,84)), transforms.ToTensor()])\n",
    "        self.stacked_queue = deque(maxlen=stack_size)\n",
    "        self.device = device\n",
    "        self.stack_size = stack_size\n",
    "        self.MemoryItem = namedtuple('MemoryItem', ['state', 'action', 'reward', 'next_state', 'not_done'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def _transform(self, x):\n",
    "        img = PIL.Image.fromarray(x)\n",
    "        img_cropped = transforms.functional.crop(img,30,30,70,100)\n",
    "        return self.img_transforms(img_cropped).to(self.device)\n",
    "    \n",
    "    def _reset_stacked_queue(self, x):\n",
    "        # reset stacked queue to the same frame\n",
    "        for _ in range(self.stack_size):\n",
    "            self.stacked_queue.append(x)\n",
    "            \n",
    "    def _get_stacked_state(self):\n",
    "        return torch.cat(list(self.stacked_queue), 0)\n",
    "            \n",
    "    def _handle_state(self, state, reset_stack):\n",
    "        if not reset_stack:\n",
    "            return self._get_stacked_state()\n",
    "        else:\n",
    "            state = self._transform(state)\n",
    "            self._reset_stacked_queue(state)\n",
    "            self.stacked_queue.append(state)\n",
    "            return self._get_stacked_state()\n",
    "        \n",
    "    def _combine_memories(self, memories):\n",
    "        states, actions, rewards, next_states, not_dones = zip(*memories)\n",
    "        return self.MemoryItem(torch.stack(states,0).to(self.device),\n",
    "                               torch.LongTensor(actions).to(self.device), \n",
    "                               torch.FloatTensor(rewards).to(self.device),\n",
    "                               torch.stack(next_states,0).to(self.device),\n",
    "                               torch.FloatTensor(not_dones).to(self.device))\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, not_done, reset_stack=False):\n",
    "        stacked_state = self._handle_state(state, reset_stack)\n",
    "        next_state = self._transform(next_state)\n",
    "        self.stacked_queue.append(next_state)\n",
    "        stacked_next_state = self._get_stacked_state()\n",
    "        self.memory.append(self.MemoryItem(stacked_state, action, reward, stacked_next_state, not_done))\n",
    "        \n",
    "    def process_single_frame(self, frame, reset=False):\n",
    "        frame = self._transform(frame)\n",
    "        if reset:\n",
    "            self._reset_stacked_queue(frame)\n",
    "            return self._get_stacked_state().unsqueeze(0)\n",
    "        else:\n",
    "            stacked_current_state = self._get_stacked_state()\n",
    "            stacked_current_state = torch.cat([stacked_current_state, frame])\n",
    "            return stacked_current_state[1:].unsqueeze(0)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indxs = np.random.choice(range(len(self.memory)), batch_size, replace=False)\n",
    "        return self._combine_memories([self.memory[i] for i in indxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tset memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(device)\n",
    "state_0 = np.random.randn(120,160)\n",
    "state_1 = np.random.randn(120,160)\n",
    "state_2 = np.random.randn(120,160)\n",
    "state_3 = np.random.randn(120,160)\n",
    "state_4 = np.random.randn(120,160)\n",
    "action = [1,0,0]\n",
    "reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.push(state_0, action, reward, state_1, True, True)\n",
    "memory.push(state_1, action, reward, state_2, True, False)\n",
    "memory.push(state_2, action, reward, state_3, True, False)\n",
    "memory.push(state_3, action, reward, state_4, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.memory[2].next_state == memory.memory[3].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0211,  0.6046,  0.1654,  ..., -0.0550,  0.3353,  0.1904],\n",
       "         [-0.1235, -0.5893, -0.2824,  ..., -0.1125,  0.5230,  0.7682],\n",
       "         [ 0.1810, -0.2502,  0.0145,  ..., -0.3502,  0.4230,  1.1999],\n",
       "         ...,\n",
       "         [-0.0371, -0.0507, -0.5456,  ..., -0.2075, -0.4384,  0.1539],\n",
       "         [ 0.1679, -0.1722, -0.3731,  ...,  0.1802,  0.0685,  0.1248],\n",
       "         [-0.2377,  0.2080, -0.3080,  ...,  0.9255,  1.9806,  0.2591]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory._transform(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4839, -0.5235,  0.1872,  ..., -0.1331, -0.3297, -1.4940],\n",
       "          [ 0.1710,  0.0037,  1.5288,  ...,  0.0493,  0.5869,  0.1274],\n",
       "          [ 0.5322,  0.2382,  0.9302,  ...,  0.7046,  0.3409, -0.4073],\n",
       "          ...,\n",
       "          [ 0.9348,  0.7462,  0.8188,  ..., -0.2516,  0.6897,  0.6333],\n",
       "          [ 0.8055,  1.8044,  0.9465,  ..., -0.7934, -0.2109,  0.8585],\n",
       "          [ 0.4826,  1.1341,  0.1587,  ...,  0.3136,  1.1907,  0.4419]],\n",
       "\n",
       "         [[-0.4839, -0.5235,  0.1872,  ..., -0.1331, -0.3297, -1.4940],\n",
       "          [ 0.1710,  0.0037,  1.5288,  ...,  0.0493,  0.5869,  0.1274],\n",
       "          [ 0.5322,  0.2382,  0.9302,  ...,  0.7046,  0.3409, -0.4073],\n",
       "          ...,\n",
       "          [ 0.9348,  0.7462,  0.8188,  ..., -0.2516,  0.6897,  0.6333],\n",
       "          [ 0.8055,  1.8044,  0.9465,  ..., -0.7934, -0.2109,  0.8585],\n",
       "          [ 0.4826,  1.1341,  0.1587,  ...,  0.3136,  1.1907,  0.4419]],\n",
       "\n",
       "         [[-0.4839, -0.5235,  0.1872,  ..., -0.1331, -0.3297, -1.4940],\n",
       "          [ 0.1710,  0.0037,  1.5288,  ...,  0.0493,  0.5869,  0.1274],\n",
       "          [ 0.5322,  0.2382,  0.9302,  ...,  0.7046,  0.3409, -0.4073],\n",
       "          ...,\n",
       "          [ 0.9348,  0.7462,  0.8188,  ..., -0.2516,  0.6897,  0.6333],\n",
       "          [ 0.8055,  1.8044,  0.9465,  ..., -0.7934, -0.2109,  0.8585],\n",
       "          [ 0.4826,  1.1341,  0.1587,  ...,  0.3136,  1.1907,  0.4419]],\n",
       "\n",
       "         [[-0.0211,  0.6046,  0.1654,  ..., -0.0550,  0.3353,  0.1904],\n",
       "          [-0.1235, -0.5893, -0.2824,  ..., -0.1125,  0.5230,  0.7682],\n",
       "          [ 0.1810, -0.2502,  0.0145,  ..., -0.3502,  0.4230,  1.1999],\n",
       "          ...,\n",
       "          [-0.0371, -0.0507, -0.5456,  ..., -0.2075, -0.4384,  0.1539],\n",
       "          [ 0.1679, -0.1722, -0.3731,  ...,  0.1802,  0.0685,  0.1248],\n",
       "          [-0.2377,  0.2080, -0.3080,  ...,  0.9255,  1.9806,  0.2591]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_normal(m.weight)\n",
    "        \n",
    "class DeepQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(8*8*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = nn.functional.elu(self.bn1(self.conv1(x)))\n",
    "        out_2 = nn.functional.elu(self.bn2(self.conv2(out_1)))\n",
    "        out_3 = nn.functional.elu(self.bn3(self.conv3(out_2)))\n",
    "        out_4 = nn.functional.elu(self.fc1(out_3.view(x.shape[0], -1)))\n",
    "        out_5 = self.fc2(out_4)\n",
    "        return out_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Deep Q Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_new_game(game):\n",
    "    \"\"\"\n",
    "    seems to work\n",
    "    \"\"\"\n",
    "    game.new_episode()\n",
    "    game_start = True\n",
    "    state = game.get_state().screen_buffer\n",
    "    return game, state, game_start\n",
    "\n",
    "def take_action(game, action):\n",
    "    \"\"\"\n",
    "    seems to work\n",
    "    \"\"\"\n",
    "    return game.make_action(action), game.is_episode_finished()\n",
    "\n",
    "def handle_done(game, state, action, reward, game_start, memory):\n",
    "    next_state = np.zeros(state.shape)\n",
    "    memory.push(state, action, reward, next_state, False, game_start)\n",
    "    \n",
    "def handle_not_done(game, state, action, reward, game_start, memory):\n",
    "    next_state = game.get_state().screen_buffer\n",
    "    memory.push(state, action, reward, next_state, True, game_start)\n",
    "    return next_state, False\n",
    "\n",
    "def initalize_memory(pretrain_length, game, possible_actions, memory):\n",
    "\n",
    "    game, state, game_start = start_new_game(game)\n",
    "    \n",
    "    for i in range(pretrain_length):\n",
    "\n",
    "        # Random action\n",
    "        action = random.choice(possible_actions)\n",
    "        reward, done = take_action(game, action)\n",
    "\n",
    "        # If we're dead\n",
    "        if done:\n",
    "            handle_done(game, state, action, reward, game_start, \n",
    "                        memory)\n",
    "            game, state, game_start = start_new_game(game)\n",
    "\n",
    "        else:\n",
    "            state, game_start = handle_not_done(game, state, action, \n",
    "                                                reward, game_start,\n",
    "                                                memory)\n",
    "\n",
    "def epsilon_greedy_move(game, model, state, possible_actions, train_params, steps_done):\n",
    "        eps_threshold = train_params.explore_stop + (train_params.explore_start - train_params.explore_stop) \\\n",
    "            * math.exp(-1. * steps_done / train_params.decay_rate)\n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                action = possible_actions[torch.argmax(model(state))]\n",
    "        else:\n",
    "            action = random.choice(possible_actions)\n",
    "        reward, done = take_action(game, action)\n",
    "        return reward, action, done, eps_threshold\n",
    "    \n",
    "def update_model(model, memory, optim, train_params):\n",
    "    \"\"\"\n",
    "    target is being calculated correctly\n",
    "    selecting max q correctly\n",
    "    correct action index being selected\n",
    "    selecting correct q value from model (predicted)\n",
    "    \"\"\"\n",
    "    memory_sample = memory.sample(train_params.batch_size)\n",
    "    max_q_next_state, _ = torch.max(model(memory_sample.next_state),1)\n",
    "    target = memory_sample.reward + (train_params.gamma * max_q_next_state * memory_sample.not_done)\n",
    "    action_indexes = torch.argmax(memory_sample.action,1).unsqueeze(1)\n",
    "    action_indexes = action_indexes.to(train_params.device)\n",
    "    predicted = model(memory_sample.state).gather(1, action_indexes).squeeze(1)\n",
    "    loss = nn.functional.mse_loss(predicted, target)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(game, model, memory, possible_actions, train_params,\n",
    "         print_every=100):\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    total_steps = 0\n",
    "    losses = []\n",
    "    for episode in range(train_params.total_episodes):\n",
    "        game, state, game_start = start_new_game(game)\n",
    "        for step in range(train_params.max_steps):\n",
    "            \n",
    "            if step == 0:\n",
    "                processed_state = memory.process_single_frame(state, True)\n",
    "            else:\n",
    "                processed_state = memory.process_single_frame(state)\n",
    "            \n",
    "            reward, action, done, eps_threshold = epsilon_greedy_move(game, model, processed_state, \n",
    "                                                              possible_actions,\n",
    "                                                              train_params, total_steps)\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                handle_done(game, state, action, reward, game_start, memory)\n",
    "                break\n",
    "            else:\n",
    "                state, game_start = handle_not_done(game, state, action, reward, game_start, memory)  \n",
    "                \n",
    "            loss = update_model(model, memory, optim, train_params)\n",
    "            \n",
    "            if total_steps % print_every == 0:\n",
    "                losses.append(loss)\n",
    "                print(\"Loss: {}\".format(loss))\n",
    "                print(\"Explore Prob: {}\".format(eps_threshold))\n",
    "                print(\"Epoch: {}\".format(episode))\n",
    "                \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7599\n",
      "***\n",
      "5880\n",
      "***\n",
      "2595\n",
      "***\n",
      "5932\n",
      "***\n",
      "2557\n",
      "***\n",
      "10113\n",
      "***\n",
      "1960\n",
      "***\n",
      "6895\n",
      "***\n",
      "5239\n",
      "***\n",
      "2520\n",
      "***\n",
      "9278\n",
      "***\n",
      "7776\n",
      "***\n",
      "7005\n",
      "***\n",
      "7533\n",
      "***\n",
      "6037\n",
      "***\n",
      "6522\n",
      "***\n",
      "8540\n",
      "***\n",
      "9896\n",
      "***\n",
      "9771\n",
      "***\n",
      "10618\n",
      "***\n",
      "8622\n",
      "***\n",
      "5392\n",
      "***\n",
      "5363\n",
      "***\n",
      "2988\n",
      "***\n",
      "8582\n",
      "***\n",
      "3227\n",
      "***\n",
      "9145\n",
      "***\n",
      "7071\n",
      "***\n",
      "6104\n",
      "***\n",
      "2810\n",
      "***\n",
      "1077\n",
      "***\n",
      "6938\n",
      "***\n",
      "9886\n",
      "***\n",
      "10099\n",
      "***\n",
      "10734\n",
      "***\n",
      "8265\n",
      "***\n",
      "7909\n",
      "***\n",
      "320\n",
      "***\n",
      "6803\n",
      "***\n",
      "11097\n",
      "***\n",
      "7070\n",
      "***\n",
      "3091\n",
      "***\n",
      "4631\n",
      "***\n",
      "9314\n",
      "***\n",
      "5512\n",
      "***\n",
      "4377\n",
      "***\n",
      "9890\n",
      "***\n",
      "8677\n",
      "***\n",
      "7966\n",
      "***\n",
      "7686\n",
      "***\n",
      "10393\n",
      "***\n",
      "11114\n",
      "***\n",
      "10881\n",
      "***\n",
      "9467\n",
      "***\n",
      "4255\n",
      "***\n",
      "3925\n",
      "***\n",
      "3429\n",
      "***\n",
      "8858\n",
      "***\n",
      "7939\n",
      "***\n",
      "8014\n",
      "***\n",
      "10966\n",
      "***\n",
      "8470\n",
      "***\n",
      "8755\n",
      "***\n",
      "8951\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "# initalize all the stuff we need\n",
    "\n",
    "memory = ReplayMemory(device)\n",
    "model = DeepQ().to(device)\n",
    "model.apply(init_weights)\n",
    "game, possible_actions = create_environment()\n",
    "game.init()\n",
    "initalize_memory(pretrain_length, game, possible_actions, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.5449132919311523\n",
      "Explore Prob: 0.9124544165176698\n",
      "Epoch: 0\n",
      "Loss: 2.5871565341949463\n",
      "Explore Prob: 0.8322121905045686\n",
      "Epoch: 1\n",
      "Loss: 1.421907663345337\n",
      "Explore Prob: 0.7594356516190675\n",
      "Epoch: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9bdc558f8737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-d19234c05d1e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(game, model, memory, possible_actions, train_params, print_every)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_not_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d19234c05d1e>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(model, memory, optim, train_params)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m     \u001b[0mmemory_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmax_q_next_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q_next_state\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmemory_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0maction_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ef4f8dbe4be2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout_5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# learn\n",
    "losses = train(game, model, memory, possible_actions, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
