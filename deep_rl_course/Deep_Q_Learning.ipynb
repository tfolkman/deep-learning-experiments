{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import PIL\n",
    "from collections import namedtuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "action_size = 3              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100             # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.05            # minimum exploration probability \n",
    "decay_rate = 1024              # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False\n",
    "\n",
    "TrainingParameters = namedtuple('TrainingParameters', ['learning_rate', 'total_episodes', 'max_steps', 'batch_size',\n",
    "                                                      'explore_start', 'explore_stop', 'decay_rate', 'gamma'])\n",
    "train_params = TrainingParameters(learning_rate, total_episodes, max_steps, batch_size, explore_start,\n",
    "                                 explore_stop, decay_rate, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Class that handles the replay buffer and takes in the raw numpy and transforms it as necessary\n",
    "    Transforms:\n",
    "        Crop out the unnecessary parts of image\n",
    "        Normalize to 0-1\n",
    "        Resize to 84x84\n",
    "    Also handles stacking the frames together\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, stack_size=4, memory_size = 1000000):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize((84,84)), transforms.ToTensor()])\n",
    "        self.stacked_queue = deque(maxlen=stack_size)\n",
    "        self.device = device\n",
    "        self.stack_size = stack_size\n",
    "        self.MemoryItem = namedtuple('MemoryItem', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def _transform(self, x):\n",
    "        img = PIL.Image.fromarray(x)\n",
    "        img_cropped = transforms.functional.crop(img,30,30,70,100)\n",
    "        return self.img_transforms(img_cropped).to(device)\n",
    "    \n",
    "    def _reset_stacked_queue(self, x):\n",
    "        # reset stacked queue to the same frame\n",
    "        for _ in range(self.stack_size):\n",
    "            self.stacked_queue.append(x)\n",
    "            \n",
    "    def _get_stacked_state(self):\n",
    "        return torch.cat(list(self.stacked_queue), 0)\n",
    "            \n",
    "    def _handle_state(self, state, reset_stack):\n",
    "        if not reset_stack:\n",
    "            return self._get_stacked_state()\n",
    "        else:\n",
    "            state = self._transform(state)\n",
    "            self._reset_stacked_queue(state)\n",
    "            self.stacked_queue.append(state)\n",
    "            return self._get_stacked_state()\n",
    "        \n",
    "    def _combine_memories(self, memories):\n",
    "        states, actions, rewards, next_states, dones = zip(*memories)\n",
    "        return self.MemoryItem(torch.stack(states,0), torch.LongTensor(actions), torch.FloatTensor(rewards), \n",
    "                               torch.stack(next_states,0), list(dones))\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done, reset_stack=False):\n",
    "        stacked_state = self._handle_state(state, reset_stack)\n",
    "        next_state = self._transform(next_state)\n",
    "        self.stacked_queue.append(next_state)\n",
    "        stacked_next_state = self._get_stacked_state()\n",
    "        self.memory.append(self.MemoryItem(stacked_state, action, reward, stacked_next_state, done))\n",
    "        \n",
    "    def process_single_frame(self, frame, reset=False):\n",
    "        frame = self._transform(frame)\n",
    "        if reset:\n",
    "            self._reset_stacked_queue(frame)\n",
    "            return self._get_stacked_state().unsqueeze(0)\n",
    "        else:\n",
    "            stacked_current_state = self._get_stacked_state()\n",
    "            stacked_current_state = torch.cat([stacked_current_state, frame])\n",
    "            return stacked_current_state[1:].unsqueeze(0)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indxs = np.random.choice(range(len(self.memory)), batch_size, replace=False)\n",
    "        return self._combine_memories([self.memory[i] for i in indxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(8*8*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = nn.functional.elu(self.bn1(self.conv1(x)))\n",
    "        out_2 = nn.functional.elu(self.bn2(self.conv2(out_1)))\n",
    "        out_3 = nn.functional.elu(self.bn3(self.conv3(out_2)))\n",
    "        out_4 = nn.functional.elu(self.fc1(out_3.view(x.shape[0], -1)))\n",
    "        out_5 = self.fc2(out_4)\n",
    "        return out_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Deep Q Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_new_game(game):\n",
    "    game.new_episode()\n",
    "    game_start = True\n",
    "    state = game.get_state().screen_buffer\n",
    "    return game, state, game_start\n",
    "\n",
    "def take_action(game, action):\n",
    "    return game.make_action(action), game.is_episode_finished()\n",
    "\n",
    "def handle_done(game, state, action, reward, game_start, memory):\n",
    "    next_state = np.zeros(state.shape)\n",
    "    memory.push(state, action, reward, next_state, True, game_start)\n",
    "    \n",
    "def handle_not_done(game, state, action, reward, game_start, memory):\n",
    "    next_state = game.get_state().screen_buffer\n",
    "    memory.push(state, action, reward, next_state, False, game_start)\n",
    "    return next_state, False\n",
    "\n",
    "\n",
    "def initalize_memory(pretrain_length, game, possible_actions, memory):\n",
    "\n",
    "    game, state, game_start = start_new_game(game)\n",
    "    \n",
    "    for i in range(pretrain_length):\n",
    "\n",
    "        # Random action\n",
    "        action = random.choice(possible_actions)\n",
    "        reward, done = take_action(game, action)\n",
    "\n",
    "        # If we're dead\n",
    "        if done:\n",
    "            handle_done(game, state, action, reward, game_start, memory)\n",
    "            game, state, game_start = start_new_game(game)\n",
    "\n",
    "        else:\n",
    "            state, game_start = handle_not_done(game, state, action, reward, game_start, memory)\n",
    "\n",
    "def epsilon_greedy_move(game, model, state, possible_actions, train_params, steps_done):\n",
    "        eps_threshold = train_params.explore_stop + (train_params.explore_start - train_params.explore_stop) \\\n",
    "            * math.exp(-1. * steps_done / train_params.decay_rate)\n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                action = possible_actions[torch.argmax(model(state))]\n",
    "        else:\n",
    "            action = random.choice(possible_actions)\n",
    "        reward, done = take_action(game, action)\n",
    "        return reward, action, done, eps_threshold\n",
    "    \n",
    "def update_model(model, memory, optim, train_params):\n",
    "    \n",
    "    memory_sample = memory.sample(train_params.batch_size)\n",
    "    max_q_next_state, _ = torch.max(model(memory_sample.next_state),1)\n",
    "    target = memory_sample.reward + train_params.gamma * max_q_next_state\n",
    "    action_indexes = torch.argmax(memory_sample.action,1).unsqueeze(1)\n",
    "    predicted = model(memory_sample.state).gather(1, action_indexes)\n",
    "    loss = torch.mean(train_params.learning_rate * (target - predicted))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "def train(game, model, memory, possible_actions, train_params):\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    total_steps = 0\n",
    "    for episode in range(train_params.max_steps):\n",
    "        game, state, game_start = start_new_game(game)\n",
    "        for step in range(train_params.max_steps):\n",
    "            \n",
    "            if step == 0:\n",
    "                processed_state = memory.process_single_frame(state, True)\n",
    "            else:\n",
    "                processed_state = memory.process_single_frame(state)\n",
    "            \n",
    "            reward, action, done, eps_threshold = epsilon_greedy_move(game, model, processed_state, \n",
    "                                                              possible_actions,\n",
    "                                                              train_params, total_steps)\n",
    "            print(total_steps)\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                handle_done(game, state, action, reward, game_start, memory)\n",
    "                break\n",
    "            else:\n",
    "                state, game_start = handle_not_done(game, state, action, reward, game_start, memory)  \n",
    "                \n",
    "            update_model(model, memory, optim, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize all the stuff we need\n",
    "\n",
    "memory = ReplayMemory(device)\n",
    "model = DeepQ().to(device)\n",
    "game, possible_actions = create_environment()\n",
    "game.init()\n",
    "initalize_memory(pretrain_length, game, possible_actions, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-0.00024397511151619256\n",
      "1\n",
      "0.0005126592586748302\n",
      "2\n",
      "0.0005919542163610458\n",
      "3\n",
      "0.0007547263521701097\n",
      "4\n",
      "0.000255617022048682\n",
      "5\n",
      "6.494517583632842e-05\n",
      "6\n",
      "1.926663753692992e-05\n",
      "7\n",
      "-0.00013910284906160086\n",
      "8\n",
      "-0.00012161706399638206\n",
      "9\n",
      "-0.00015464304306078702\n",
      "10\n",
      "-0.00020955948275513947\n",
      "11\n",
      "-0.0002492075727786869\n",
      "12\n",
      "-0.0001538627257104963\n",
      "13\n",
      "-0.0002411362511338666\n",
      "14\n",
      "-0.00030555433477275074\n",
      "15\n",
      "-0.00030491568031720817\n",
      "16\n",
      "-0.0002868709561880678\n",
      "17\n",
      "-0.0003337352245580405\n",
      "18\n",
      "-0.00032306695356965065\n",
      "19\n",
      "-0.0003163553774356842\n",
      "20\n",
      "-0.00032691197702661157\n",
      "21\n",
      "-0.00036558459396474063\n",
      "22\n",
      "-0.0003617813636083156\n",
      "23\n",
      "-0.00037732257624156773\n",
      "24\n",
      "-0.0002816447231452912\n",
      "25\n",
      "-0.00041453776066191494\n",
      "26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-757ec765792e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-185-d35ff9b960a4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(game, model, memory, possible_actions, train_params)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_not_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-185-d35ff9b960a4>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(model, memory, optim, train_params)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tfolkman/anaconda3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tfolkman/anaconda3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# learn\n",
    "train(game, model, memory, possible_actions, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
