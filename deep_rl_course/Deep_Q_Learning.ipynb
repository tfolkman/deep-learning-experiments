{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import PIL\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "action_size = 3              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100             # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Class that handles the replay buffer and takes in the raw numpy and transforms it as necessary\n",
    "    Transforms:\n",
    "        Crop out the unnecessary parts of image\n",
    "        Normalize to 0-1\n",
    "        Resize to 84x84\n",
    "    Also handles stacking the frames together\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, stack_size=4, memory_size = 1000000):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.img_transforms = transforms.Compose([transforms.Resize((84,84)), transforms.ToTensor()])\n",
    "        self.stacked_queue = deque(maxlen=stack_size)\n",
    "        self.device = device\n",
    "        self.stack_size = stack_size\n",
    "        self.MemoryItem = namedtuple('MemoryItem', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def _transform(self, x):\n",
    "        img = PIL.Image.fromarray(x)\n",
    "        img_cropped = transforms.functional.crop(img,30,30,70,100)\n",
    "        return self.img_transforms(img_cropped).to(device)\n",
    "    \n",
    "    def _reset_stacked_queue(self, x):\n",
    "        # reset stacked queue to the same frame\n",
    "        for _ in range(self.stack_size-1):\n",
    "            self.stacked_queue.append(x)\n",
    "            \n",
    "    def _get_stacked_state(self):\n",
    "        return torch.cat(list(self.stacked_queue), 0)\n",
    "            \n",
    "    def _handle_state(self, state, reset_stack):\n",
    "        if state is None:\n",
    "            return self._get_stacked_state()\n",
    "        else:\n",
    "            state = self._transform(state)\n",
    "            if reset_stack:\n",
    "                self._reset_stacked_queue(state)\n",
    "            self.stacked_queue.append(state)\n",
    "            return self._get_stacked_state()\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done, reset_stack=False):\n",
    "        stacked_state = self._handle_state(state)\n",
    "        next_state = self._transform(next_state)\n",
    "        self.stacked_queue.append(next_state)\n",
    "        stacked_next_state = self._get_stacked_state()\n",
    "        self.memory.append(self.MemoryItem(stacked_state, action, reward, stacked_next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indxs = np.random.choice(range(len(self.memory)), batch_size, replace=False)\n",
    "        return torch.stack([self.memory[i] for i in indxs], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(device)\n",
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.new_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.get_state().screen_buffer\n",
    "for i in range(pretrain_length):\n",
    "        \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQ, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(8*8*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = nn.functional.elu(self.bn1(self.conv1(x)))\n",
    "        out_2 = nn.functional.elu(self.bn2(self.conv2(out_1)))\n",
    "        out_3 = nn.functional.elu(self.bn3(self.conv3(out_2)))\n",
    "        out_4 = nn.functional.elu(self.fc1(out_3.view(1, -1)))\n",
    "        out_5 = self.fc2(out_4)\n",
    "        return out_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Deep Q Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_memory(pretrain_length, game):\n",
    "    game.new_episode()\n",
    "    for i in range(pretrain_length):\n",
    "        take_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()\n",
    "model = DeepQ().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepQ().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay = ReplayMemory(device)\n",
    "replay.push(np.random.rand(120,160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1118,  0.3007,  0.1629]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(replay.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
