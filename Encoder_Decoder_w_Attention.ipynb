{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bastings.github.io/annotated_encoder_decoder/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "from torchtext import data, datasets\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Encoder-Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "              decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                           src_mask, trg_mask, hidden=decoder_hidden)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Use bi-direction GRU\n",
    "\n",
    "For efficiency, we need to support mini-batches. Sentences may have different lengths, so unroll differently. \n",
    "\n",
    "pack_padded_sequence - Packs a Tensor containing padded sequences of variable length\n",
    "pad_packed_sequence - undoes the pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                         batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bi-directional GRU to sequence of embeddings X\n",
    "        The input mini-batch x needs to be sorted by length\n",
    "        x should have dimensions [batch, seq_length, input_size]\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        # final shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # the first dimension will be a multiple of 2 when bi-directional\n",
    "        # for example, with num_layers = 2, then first dimension is 4\n",
    "        # the 0th and 2st rows and the final forwards and\n",
    "        # the 1st and 3rd rows the final backwards \n",
    "        hidden, final = self.rnn(packed)\n",
    "        # also returns the lengths\n",
    "        # shape of hidden: (batch, seq_length, num_directions*hidden size)\n",
    "        hidden, _ = pad_packed_sequence(hidden, batch_first=True)\n",
    "        \n",
    "        #get all final forwards\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        #get all final backwards\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        # shape of final: (num layers, batch, 2*hidden_size)\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)\n",
    "        \n",
    "        return hidden, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "We will always use teacher forcing\n",
    "\n",
    "Encoder Final is the last hidden state used to initalize first hidden state of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # will concat the previous word embedding with the context\n",
    "        # where the context is the weighted sum of the encoder hidden states\n",
    "        # which has a size of 2 * hidden\n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                         batch_first=True, dropout=dropout)\n",
    "        # to initalize from the final encoder state\n",
    "        # need to project because goes from bi-directional size to one direction\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                         hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Preform a single decoder step - 1 word\"\"\"\n",
    "        \n",
    "        # this is the hidden state for the decoder\n",
    "        query = hidden[-1].unsqueeze(1) # takes the last layer. shape: [batch, 1, hidden_dimension]\n",
    "        context, _ = self.attention(query=query, proj_key=proj_key,\n",
    "                                            value=encoder_hidden, mask=src_mask)\n",
    "        \n",
    "        # process the concat of the prev_embedding and the context (attention)\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # use the prev_embed, output from GRU, and context to get final vector of hidden_size\n",
    "        # which will be used to generated probabilities across our vocabulary\n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "        \n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, src_mask, trg_mask,\n",
    "               hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the encoder 1 step at a time\"\"\"\n",
    "        \n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "            \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "            \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        # projects them to hidden_dimension size\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "    \n",
    "        \n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(prev_embed,\n",
    "                                                          encoder_hidden,\n",
    "                                                          src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "            \n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        # all we really use are the pre_output_vectors\n",
    "        return decoder_states, hidden, pre_output_vectors # [B,N,D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the intial decoder state, conditioned on final encoder state\"\"\"\n",
    "        if encoder_final is None:\n",
    "            return None #start with zeros\n",
    "        else:\n",
    "            return torch.tanh(self.bridge(encoder_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "At every time step, decoder has **ALL** the source word hidden states. Attention allows it to learn which are the most relevant. \n",
    "\n",
    "The state of the decoder is represented by the hidden state.\n",
    "\n",
    "We will use an MLP-based, additive attention with tanh activation.\n",
    "\n",
    "**Decoder state:** is the query\n",
    "**The encoder states:** the key\n",
    "\n",
    "We add the query to all of the keys and pass them through a tanh function then project them to a single number (the **energy**).\n",
    "\n",
    "We then mask out invalid positions and apply softmax to get probability distribution across the words.\n",
    "\n",
    "We then take a weighted sum of the encoder hidden states, where the weights are the probabilities. This is the **context.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # assume bi-directional encoder \n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "        \n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "        \n",
    "        # project the query to hidden_size (this already done for keys)\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate energies\n",
    "        # pytorch broadcasts the query to all the keys with the addition\n",
    "        # shape batch x seq_length x 1\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        # batch x 1 x seq_length\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        # the mask is broadcastable b/c it is of size seq_length\n",
    "        # see: https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics\n",
    "        # the softmax will set all negative inf values to zero\n",
    "        # thus wont be used in weighted value\n",
    "        # which is appropriate since they are padding values\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # turn scores into probabilities\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        # context is weighted sum of the values (the original encoder hidden states)\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: batch x 1 x key size\n",
    "        # alpha shape: batch x 1 x mask length\n",
    "        return context, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English to French Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load('fr')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "\n",
    "MAX_LEN = 25\n",
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up fields and load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_de, batch_first=True, lower=LOWER, \n",
    "                  include_lengths=True, unk_token=UNK_TOKEN, \n",
    "                  pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    \n",
    "TRG = data.Field(tokenize=tokenize_en, batch_first=True, lower=LOWER, \n",
    "                 include_lengths=True, unk_token=UNK_TOKEN, \n",
    "                 pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading fr-en.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fr-en.tgz: 100%|██████████| 25.7M/25.7M [00:15<00:00, 1.72MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".data/iwslt/fr-en/IWSLT16.TED.dev2010.fr-en.en.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2012.fr-en.fr.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2013.fr-en.en.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2010.fr-en.fr.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2010.fr-en.en.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2014.fr-en.fr.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2014.fr-en.en.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.dev2010.fr-en.fr.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2013.fr-en.fr.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2012.fr-en.en.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2011.fr-en.fr.xml\n",
      ".data/iwslt/fr-en/IWSLT16.TED.tst2011.fr-en.en.xml\n",
      ".data/iwslt/fr-en/train.tags.fr-en.fr\n",
      ".data/iwslt/fr-en/train.tags.fr-en.en\n"
     ]
    }
   ],
   "source": [
    "def data_filter(x):\n",
    "    len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "train_data, valid_data, test_data = datasets.IWSLT.splits(exts=('.fr', '.en'), \n",
    "                                    fields=(SRC, TRG), \n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    \n",
    "PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "Use bucketIterator which will group sentences by close to same length as possible. For efficiency, pytorch requires batches sorted by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=64, train=True,\n",
    "                                sort_within_batch=True,\n",
    "                                sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                                repeat=False, device=DEVICE)\n",
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, \n",
    "                           repeat=False, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"\n",
    "    Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        # the bucket iterator returns a tuple of (src, src_lengths)\n",
    "        # shape of src: bxmax length\n",
    "        # shape of src_lengths: b\n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        # shape: b x 1 x max_length\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "        \n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            # -1 b/c these are the inputs\n",
    "            self.trg = trg[:,:-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            # 1 b/c these the what we will predict\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # shape: b x max length\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "\n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "            \n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()\n",
    "                \n",
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        # get log_softmax over the vocab\n",
    "        # shape x: batch x seq_length x vocab_size\n",
    "        # shape y: batch x seq_length (the truth)\n",
    "        x = self.generator(x)\n",
    "        \n",
    "        # our nllloss expects the x input to be NxC where C is the vocab size\n",
    "        # and y input to be N\n",
    "        # so we flatten them out\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking and Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # encode the src\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_lengths)\n",
    "        # prev_y starts as sos_index \n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        # the target mask is all ones b/c all valid\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    # cut off everything starting from </s> \n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]\n",
    "\n",
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    # this will never happen in our use case\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "        \n",
    "        # get numpy versions of tensors\n",
    "        # trg is trg_y b/c that's the truth and already skips the SOS token\n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model\n",
    "\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        \n",
    "        _, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                        batch.src_mask, batch.trg_mask,\n",
    "                                        batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "            \n",
    "    return math.exp(total_loss / float(total_tokens))\n",
    "\n",
    "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "        \n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dev_perplexities = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch\", epoch)\n",
    "        model.train()\n",
    "        # we using () as a generator comprehension\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter),\n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator,\n",
    "                                                      criterion,\n",
    "                                                      optim),\n",
    "                                     print_every=print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
    "\n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "    return dev_perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 500 Loss: 38.328526 Tokens per Sec: 19878.745069\n",
      "Epoch Step: 1000 Loss: 74.257416 Tokens per Sec: 19896.994471\n",
      "Epoch Step: 1500 Loss: 86.251877 Tokens per Sec: 20215.667041\n",
      "Epoch Step: 2000 Loss: 97.226913 Tokens per Sec: 20009.305914\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i took a <unk> for a <unk> and every single that we 've been in africa .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i was , i was the <unk> , we 're going to do people who are going to be able to be able to work .\n",
      "\n",
      "Validation perplexity: 24.872736\n",
      "Epoch 1\n",
      "Epoch Step: 500 Loss: 19.933880 Tokens per Sec: 19738.261318\n",
      "Epoch Step: 1000 Loss: 35.622902 Tokens per Sec: 19727.337476\n",
      "Epoch Step: 1500 Loss: 51.796204 Tokens per Sec: 19640.864769\n",
      "Epoch Step: 2000 Loss: 71.215225 Tokens per Sec: 20108.365451\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked for a <unk> and every project that we 've been in africa in africa .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really absolutely .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , in years , that we were doing people to be people and we 're going to be good at all of africa .\n",
      "\n",
      "Validation perplexity: 14.489458\n",
      "Epoch 2\n",
      "Epoch Step: 500 Loss: 74.837250 Tokens per Sec: 19864.830981\n",
      "Epoch Step: 1000 Loss: 31.026478 Tokens per Sec: 20120.031892\n",
      "Epoch Step: 1500 Loss: 62.885983 Tokens per Sec: 20171.557753\n",
      "Epoch Step: 2000 Loss: 32.911282 Tokens per Sec: 19980.600240\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked for a <unk> and every project that we 've been in africa has failed .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really just really .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , 21 years , that we were the people of people and we were using good working in africa .\n",
      "\n",
      "Validation perplexity: 11.192585\n",
      "Epoch 3\n",
      "Epoch Step: 500 Loss: 64.837631 Tokens per Sec: 19927.811737\n",
      "Epoch Step: 1000 Loss: 32.400272 Tokens per Sec: 20346.663670\n",
      "Epoch Step: 1500 Loss: 18.542820 Tokens per Sec: 20166.819757\n",
      "Epoch Step: 2000 Loss: 24.065329 Tokens per Sec: 20233.050232\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked for a <unk> and the project that we 've been in africa has failed .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really just really .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , 21 years , we were sending them to be goods and we 're designing in the work in africa .\n",
      "\n",
      "Validation perplexity: 9.912346\n",
      "Epoch 4\n",
      "Epoch Step: 500 Loss: 50.822777 Tokens per Sec: 20073.770468\n",
      "Epoch Step: 1000 Loss: 47.582600 Tokens per Sec: 20058.012974\n",
      "Epoch Step: 1500 Loss: 51.201717 Tokens per Sec: 20141.705811\n",
      "Epoch Step: 2000 Loss: 51.364258 Tokens per Sec: 19950.952981\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked for a <unk> and every project that we mounted in africa has failed .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really upset .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i was in the age of years , that we were sending people and they were <unk> in the world .\n",
      "\n",
      "Validation perplexity: 9.312477\n",
      "Epoch 5\n",
      "Epoch Step: 500 Loss: 57.760780 Tokens per Sec: 20059.912840\n",
      "Epoch Step: 1000 Loss: 17.558708 Tokens per Sec: 20200.854723\n",
      "Epoch Step: 1500 Loss: 19.499462 Tokens per Sec: 20325.743300\n",
      "Epoch Step: 2000 Loss: 35.919628 Tokens per Sec: 19909.747066\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i 've been working for a ngo and the <unk> that we mounted in africa has failed .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really upset .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , in 21 years , we were sending them to be empty goods and we 're doing good job in africa .\n",
      "\n",
      "Validation perplexity: 8.780696\n",
      "Epoch 6\n",
      "Epoch Step: 500 Loss: 15.203369 Tokens per Sec: 19671.194022\n",
      "Epoch Step: 1000 Loss: 31.438313 Tokens per Sec: 19609.801662\n",
      "Epoch Step: 1500 Loss: 27.906096 Tokens per Sec: 19500.873036\n",
      "Epoch Step: 2000 Loss: 26.680122 Tokens per Sec: 19497.210855\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked for a <unk> and a project that we 've been in africa in africa .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really upset .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , in 21 years , that we were sending them to come and we have a good job in africa .\n",
      "\n",
      "Validation perplexity: 8.825209\n",
      "Epoch 7\n",
      "Epoch Step: 500 Loss: 33.326580 Tokens per Sec: 19447.266949\n",
      "Epoch Step: 1000 Loss: 43.309063 Tokens per Sec: 19687.927967\n",
      "Epoch Step: 1500 Loss: 24.578161 Tokens per Sec: 19794.946105\n",
      "Epoch Step: 2000 Loss: 26.627480 Tokens per Sec: 19689.301307\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked for a ngo and the <unk> project we 've mounted in africa has failed .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really upset .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , in 21 years , that we were certainly funding them to come and lead to work in the world .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation perplexity: 8.611184\n",
      "Epoch 8\n",
      "Epoch Step: 500 Loss: 16.236769 Tokens per Sec: 19531.470778\n",
      "Epoch Step: 1000 Loss: 21.538202 Tokens per Sec: 19778.229386\n",
      "Epoch Step: 1500 Loss: 30.669321 Tokens per Sec: 20148.839575\n",
      "Epoch Step: 2000 Loss: 22.319052 Tokens per Sec: 19793.007229\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i 've been working for a chicken and every single project that we 've mounted in africa has failed .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really upset .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , in 21 years , we were making them to be a lot of goods and we were doing good at africa .\n",
      "\n",
      "Validation perplexity: 8.537399\n",
      "Epoch 9\n",
      "Epoch Step: 500 Loss: 15.230019 Tokens per Sec: 19569.429198\n",
      "Epoch Step: 1000 Loss: 29.618410 Tokens per Sec: 19544.588532\n",
      "Epoch Step: 1500 Loss: 18.019587 Tokens per Sec: 19552.107689\n",
      "Epoch Step: 2000 Loss: 41.857666 Tokens per Sec: 19913.308694\n",
      "\n",
      "Example #1\n",
      "Src :  j'ai travaillé pour une ong italienne et chaque projet que nous avons monté en afrique a échoué .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked for a <unk> .- and every project that we mounted in africa has failed .\n",
      "\n",
      "Example #2\n",
      "Src :  et j'étais vraiment bouleversé .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  and i was really upset .\n",
      "\n",
      "Example #3\n",
      "Src :  je pensais , à 21 ans , que nous les italiens étions des gens biens et que nous faisions du bon travail en afrique .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i thought , in 21 years , that we were funding to people and they were working on the right job in africa .\n",
      "\n",
      "Validation perplexity: 8.607568\n"
     ]
    }
   ],
   "source": [
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=1, dropout=0.2)\n",
    "dev_perplexities = train(model, print_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYhIjlMQiIjkOAWBiEiO+/8i31q4TO+RSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "\n",
    "plot_perplexity(dev_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [\" \".join(example.trg) for example in valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank you .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert german into english using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "alphas = []\n",
    "for batch in valid_iter:\n",
    "    batch = rebatch(PAD_INDEX, batch)\n",
    "    pred, attention = greedy_decode(model, batch.src, batch.src_mask,\n",
    "                                   batch.src_lengths, max_len=25,\n",
    "                                   sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
    "                                   eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
    "    hypotheses.append(pred)\n",
    "    alphas.append(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]\n",
    "hypotheses = [\" \".join(x) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank you .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3957473423028\n"
     ]
    }
   ],
   "source": [
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(src, trg, scores):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(scores, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels(trg, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(src, minor=False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # and the x-ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src ['et', 'puis', 'il', 'faut', 'leur', 'dire', 'la', 'vérité', 'sur', \"l'entrepreneuriat\", '.', '</s>']\n",
      "ref ['and', 'then', 'you', 'have', 'to', 'tell', 'them', 'the', 'truth', 'about', 'entrepreneurship', '.', '</s>']\n",
      "pred ['and', 'then', 'you', 'have', 'to', 'tell', 'them', 'the', 'truth', 'about', 'entrepreneurship', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAFJCAYAAADOqrnnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 75\n",
    "src = valid_data[idx].src + [\"</s>\"]\n",
    "trg = valid_data[idx].trg + [\"</s>\"]\n",
    "pred = hypotheses[idx].split() + [\"</s>\"]\n",
    "# [0] b/c only 1 row\n",
    "# .T to get src as rows\n",
    "# then limit columns to length of predicted to fit\n",
    "pred_att = alphas[idx][0].T[:, :len(pred)]\n",
    "print(\"src\", src)\n",
    "print(\"ref\", trg)\n",
    "print(\"pred\", pred)\n",
    "plot_heatmap(src, pred, pred_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
