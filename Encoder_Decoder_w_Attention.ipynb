{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bastings.github.io/annotated_encoder_decoder/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "from torchtext import data, datasets\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Encoder-Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "              decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                           src_mask, trg_mask, hidden=decoder_hidden)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Use bi-direction GRU\n",
    "\n",
    "For efficiency, we need to support mini-batches. Sentences may have different lengths, so unroll differently. \n",
    "\n",
    "pack_padded_sequence - Packs a Tensor containing padded sequences of variable length\n",
    "pad_packed_sequence - undoes the pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                         batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bi-directional GRU to sequence of embeddings X\n",
    "        The input mini-batch x needs to be sorted by length\n",
    "        x should have dimensions [batch, seq_length, input_size]\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        # final shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # the first dimension will be a multiple of 2 when bi-directional\n",
    "        # for example, with num_layers = 2, then first dimension is 4\n",
    "        # the 0th and 2st rows and the final forwards and\n",
    "        # the 1st and 3rd rows the final backwards \n",
    "        hidden, final = self.rnn(packed)\n",
    "        # also returns the lengths\n",
    "        # shape of hidden: (batch, seq_length, num_directions*hidden size)\n",
    "        hidden, _ = pad_packed_sequence(hidden, batch_first=True)\n",
    "        \n",
    "        #get all final forwards\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        #get all final backwards\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        # shape of final: (num layers, batch, 2*hidden_size)\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)\n",
    "        \n",
    "        return hidden, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "We will always use teacher forcing\n",
    "\n",
    "Encoder Final is the last hidden state used to initalize first hidden state of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # will concat the previous word embedding with the context\n",
    "        # where the context is the weighted sum of the encoder hidden states\n",
    "        # which has a size of 2 * hidden\n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                         batch_first=True, dropout=dropout)\n",
    "        # to initalize from the final encoder state\n",
    "        # need to project because goes from bi-directional size to one direction\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                         hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Preform a single decoder step - 1 word\"\"\"\n",
    "        \n",
    "        # this is the hidden state for the decoder\n",
    "        query = hidden[-1].unsqueeze(1) # takes the last layer. shape: [batch, 1, hidden_dimension]\n",
    "        context, _ = self.attention(query=query, proj_key=proj_key,\n",
    "                                            value=encoder_hidden, mask=src_mask)\n",
    "        \n",
    "        # process the concat of the prev_embedding and the context (attention)\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # use the prev_embed, output from GRU, and context to get final vector of hidden_size\n",
    "        # which will be used to generated probabilities across our vocabulary\n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "        \n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, src_mask, trg_mask,\n",
    "               hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the encoder 1 step at a time\"\"\"\n",
    "        \n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "            \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "            \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        # projects them to hidden_dimension size\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "    \n",
    "        \n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(prev_embed,\n",
    "                                                          encoder_hidden,\n",
    "                                                          src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "            \n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        # all we really use are the pre_output_vectors\n",
    "        return decoder_states, hidden, pre_output_vectors # [B,N,D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the intial decoder state, conditioned on final encoder state\"\"\"\n",
    "        if encoder_final is None:\n",
    "            return None #start with zeros\n",
    "        else:\n",
    "            return torch.tanh(self.bridge(encoder_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "At every time step, decoder has **ALL** the source word hidden states. Attention allows it to learn which are the most relevant. \n",
    "\n",
    "The state of the decoder is represented by the hidden state.\n",
    "\n",
    "We will use an MLP-based, additive attention with tanh activation.\n",
    "\n",
    "**Decoder state:** is the query\n",
    "**The encoder states:** the key\n",
    "\n",
    "We add the query to all of the keys and pass them through a tanh function then project them to a single number (the **energy**).\n",
    "\n",
    "We then mask out invalid positions and apply softmax to get probability distribution across the words.\n",
    "\n",
    "We then take a weighted sum of the encoder hidden states, where the weights are the probabilities. This is the **context.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # assume bi-directional encoder \n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "        \n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "        \n",
    "        # project the query to hidden_size (this already done for keys)\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate energies\n",
    "        # pytorch broadcasts the query to all the keys with the addition\n",
    "        # shape batch x seq_length x 1\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        # batch x 1 x seq_length\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        # the mask is broadcastable b/c it is of size seq_length\n",
    "        # see: https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics\n",
    "        # the softmax will drop all negative infinity values\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # turn scores into probabilities\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        # context is weighted sum of the values (the original encoder hidden states)\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: batch x 1 x key size\n",
    "        # alpha shape: batch x 1 x mask length\n",
    "        return context, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:\n",
    "need to figure out how masking is working and the value in attention. The should be the same dimensions for bmm to work...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English to German Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "\n",
    "MAX_LEN = 25\n",
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up fields and load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_de, batch_first=True, lower=LOWER, \n",
    "                  include_lengths=True, unk_token=UNK_TOKEN, \n",
    "                  pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    \n",
    "TRG = data.Field(tokenize=tokenize_en, batch_first=True, lower=LOWER, \n",
    "                 include_lengths=True, unk_token=UNK_TOKEN, \n",
    "                 pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filter(x):\n",
    "    len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "train_data, valid_data, test_data = datasets.IWSLT.splits(exts=('.de', '.en'), \n",
    "                                    fields=(SRC, TRG), \n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    \n",
    "PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "Use bucketIterator which will group sentences by close to same length as possible. For efficiency, pytorch requires batches sorted by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=64, train=True,\n",
    "                                sort_within_batch=True,\n",
    "                                sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                                repeat=False, device=DEVICE)\n",
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, \n",
    "                           repeat=False, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"\n",
    "    Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        # the bucket iterator returns a tuple of (src, src_lengths)\n",
    "        # shape of src: bxmax length\n",
    "        # shape of src_lengths: b\n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        # shape: b x 1 x max_length\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "        \n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            # -1 b/c these are the inputs\n",
    "            self.trg = trg[:,:-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            # 1 b/c these the what we will predict\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # shape: b x max length\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "\n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "            \n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()\n",
    "                \n",
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        # get log_softmax over the vocab\n",
    "        # shape x: batch x seq_length x vocab_size\n",
    "        # shape y: batch x seq_length (the truth)\n",
    "        x = self.generator(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        print(x.view(-1, x.size(-1)).shape)\n",
    "        print(y.shape)\n",
    "        print(y.contiguous().view(-1).shape)\n",
    "        print(\"*****\")\n",
    "        \n",
    "        # our nllloss expects the x input to be NxC where C is the vocab size\n",
    "        # and y input to be N\n",
    "        # so we flatten them out\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking and Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # encode the src\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_lengths)\n",
    "        # prev_y starts as sos_index \n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        # the target mask is all ones b/c all valid\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    # cut off everything starting from </s> \n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]\n",
    "\n",
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    # this will never happen in our use case\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "        \n",
    "        # get numpy versions of tensors\n",
    "        # trg is trg_y b/c that's the truth and already skips the SOS token\n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model\n",
    "\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        \n",
    "        _, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                        batch.src_mask, batch.trg_mask,\n",
    "                                        batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "            \n",
    "    return math.exp(total_loss / float(total_tokens))\n",
    "\n",
    "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "        \n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dev_perplexities = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch\", epoch)\n",
    "        model.train()\n",
    "        # we using () as a generator comprehension\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter),\n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator,\n",
    "                                                      criterion,\n",
    "                                                      optim),\n",
    "                                     print_every=print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
    "\n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "    return dev_perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "torch.Size([64, 16, 13003])\n",
      "torch.Size([1024, 13003])\n",
      "torch.Size([64, 16])\n",
      "torch.Size([1024])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 21, 13003])\n",
      "torch.Size([1344, 13003])\n",
      "torch.Size([64, 21])\n",
      "torch.Size([1344])\n",
      "*****\n",
      "torch.Size([64, 18, 13003])\n",
      "torch.Size([1152, 13003])\n",
      "torch.Size([64, 18])\n",
      "torch.Size([1152])\n",
      "*****\n",
      "torch.Size([64, 9, 13003])\n",
      "torch.Size([576, 13003])\n",
      "torch.Size([64, 9])\n",
      "torch.Size([576])\n",
      "*****\n",
      "torch.Size([64, 18, 13003])\n",
      "torch.Size([1152, 13003])\n",
      "torch.Size([64, 18])\n",
      "torch.Size([1152])\n",
      "*****\n",
      "torch.Size([64, 15, 13003])\n",
      "torch.Size([960, 13003])\n",
      "torch.Size([64, 15])\n",
      "torch.Size([960])\n",
      "*****\n",
      "torch.Size([64, 15, 13003])\n",
      "torch.Size([960, 13003])\n",
      "torch.Size([64, 15])\n",
      "torch.Size([960])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 12, 13003])\n",
      "torch.Size([768, 13003])\n",
      "torch.Size([64, 12])\n",
      "torch.Size([768])\n",
      "*****\n",
      "torch.Size([64, 20, 13003])\n",
      "torch.Size([1280, 13003])\n",
      "torch.Size([64, 20])\n",
      "torch.Size([1280])\n",
      "*****\n",
      "torch.Size([64, 20, 13003])\n",
      "torch.Size([1280, 13003])\n",
      "torch.Size([64, 20])\n",
      "torch.Size([1280])\n",
      "*****\n",
      "torch.Size([64, 24, 13003])\n",
      "torch.Size([1536, 13003])\n",
      "torch.Size([64, 24])\n",
      "torch.Size([1536])\n",
      "*****\n",
      "torch.Size([64, 24, 13003])\n",
      "torch.Size([1536, 13003])\n",
      "torch.Size([64, 24])\n",
      "torch.Size([1536])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 14, 13003])\n",
      "torch.Size([896, 13003])\n",
      "torch.Size([64, 14])\n",
      "torch.Size([896])\n",
      "*****\n",
      "torch.Size([64, 12, 13003])\n",
      "torch.Size([768, 13003])\n",
      "torch.Size([64, 12])\n",
      "torch.Size([768])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 6, 13003])\n",
      "torch.Size([384, 13003])\n",
      "torch.Size([64, 6])\n",
      "torch.Size([384])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 21, 13003])\n",
      "torch.Size([1344, 13003])\n",
      "torch.Size([64, 21])\n",
      "torch.Size([1344])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 8, 13003])\n",
      "torch.Size([512, 13003])\n",
      "torch.Size([64, 8])\n",
      "torch.Size([512])\n",
      "*****\n",
      "torch.Size([64, 13, 13003])\n",
      "torch.Size([832, 13003])\n",
      "torch.Size([64, 13])\n",
      "torch.Size([832])\n",
      "*****\n",
      "torch.Size([64, 9, 13003])\n",
      "torch.Size([576, 13003])\n",
      "torch.Size([64, 9])\n",
      "torch.Size([576])\n",
      "*****\n",
      "torch.Size([64, 10, 13003])\n",
      "torch.Size([640, 13003])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([640])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 12, 13003])\n",
      "torch.Size([768, 13003])\n",
      "torch.Size([64, 12])\n",
      "torch.Size([768])\n",
      "*****\n",
      "torch.Size([64, 13, 13003])\n",
      "torch.Size([832, 13003])\n",
      "torch.Size([64, 13])\n",
      "torch.Size([832])\n",
      "*****\n",
      "torch.Size([64, 16, 13003])\n",
      "torch.Size([1024, 13003])\n",
      "torch.Size([64, 16])\n",
      "torch.Size([1024])\n",
      "*****\n",
      "torch.Size([64, 11, 13003])\n",
      "torch.Size([704, 13003])\n",
      "torch.Size([64, 11])\n",
      "torch.Size([704])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 14, 13003])\n",
      "torch.Size([896, 13003])\n",
      "torch.Size([64, 14])\n",
      "torch.Size([896])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 14, 13003])\n",
      "torch.Size([896, 13003])\n",
      "torch.Size([64, 14])\n",
      "torch.Size([896])\n",
      "*****\n",
      "torch.Size([64, 16, 13003])\n",
      "torch.Size([1024, 13003])\n",
      "torch.Size([64, 16])\n",
      "torch.Size([1024])\n",
      "*****\n",
      "torch.Size([64, 10, 13003])\n",
      "torch.Size([640, 13003])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([640])\n",
      "*****\n",
      "torch.Size([64, 17, 13003])\n",
      "torch.Size([1088, 13003])\n",
      "torch.Size([64, 17])\n",
      "torch.Size([1088])\n",
      "*****\n",
      "torch.Size([64, 26, 13003])\n",
      "torch.Size([1664, 13003])\n",
      "torch.Size([64, 26])\n",
      "torch.Size([1664])\n",
      "*****\n",
      "torch.Size([64, 23, 13003])\n",
      "torch.Size([1472, 13003])\n",
      "torch.Size([64, 23])\n",
      "torch.Size([1472])\n",
      "*****\n",
      "torch.Size([64, 25, 13003])\n",
      "torch.Size([1600, 13003])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([1600])\n",
      "*****\n",
      "torch.Size([64, 14, 13003])\n",
      "torch.Size([896, 13003])\n",
      "torch.Size([64, 14])\n",
      "torch.Size([896])\n",
      "*****\n",
      "torch.Size([64, 14, 13003])\n",
      "torch.Size([896, 13003])\n",
      "torch.Size([64, 14])\n",
      "torch.Size([896])\n",
      "*****\n",
      "torch.Size([64, 10, 13003])\n",
      "torch.Size([640, 13003])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([640])\n",
      "*****\n",
      "torch.Size([64, 15, 13003])\n",
      "torch.Size([960, 13003])\n",
      "torch.Size([64, 15])\n",
      "torch.Size([960])\n",
      "*****\n",
      "torch.Size([64, 16, 13003])\n",
      "torch.Size([1024, 13003])\n",
      "torch.Size([64, 16])\n",
      "torch.Size([1024])\n",
      "*****\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b4999b9d14b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                    \u001b[0memb_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    num_layers=1, dropout=0.2)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdev_perplexities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-557ecfb69052>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, lr, print_every)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                       \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                                       optim),\n\u001b[0;32m---> 62\u001b[0;31m                                      print_every=print_every)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-557ecfb69052>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute, print_every)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         _, _, pre_output = model.forward(batch.src, batch.trg,\n",
      "\u001b[0;32m<ipython-input-43-557ecfb69052>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# we using () as a generator comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter),\n\u001b[0m\u001b[1;32m     58\u001b[0m                                      \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                      SimpleLossCompute(model.generator,\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tyler/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=1, dropout=0.2)\n",
    "dev_perplexities = train(model, num_epochs=1, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
