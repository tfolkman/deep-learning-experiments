{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bastings.github.io/annotated_encoder_decoder/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Encoder-Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "              decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                           src_mask, trg_mask, hidden=decoder_hidden)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Use bi-direction GRU\n",
    "\n",
    "For efficiency, we need to support mini-batches. Sentences may have different lengths, so unroll differently. \n",
    "\n",
    "pack_padded_sequence - Packs a Tensor containing padded sequences of variable length\n",
    "pad_packed_sequence - undoes the pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                         batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bi-directional GRU to sequence of embeddings X\n",
    "        The input mini-batch x needs to be sorted by length\n",
    "        x should have dimensions [batch, seq_length, input_size]\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        # final shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # the first dimension will be a multiple of 2 when bi-directional\n",
    "        # for example, with num_layers = 2, then first dimension is 4\n",
    "        # the 0th and 2st rows and the final forwards and\n",
    "        # the 1st and 3rd rows the final backwards \n",
    "        hidden, final = self.rnn(packed)\n",
    "        # also returns the lengths\n",
    "        # shape of hidden: (batch, seq_length, num_directions*hidden size)\n",
    "        hidden, _ = pad_packed_sequence(hidden, batch_first=True)\n",
    "        \n",
    "        #get all final forwards\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        #get all final backwards\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        # shape of final: (num layers, batch, 2*hidden_size)\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)\n",
    "        \n",
    "        return hidden, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(np.array([[[4], [2]], [[3], [1]], [[2], [5]]])).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 3 sentences of 2 words each\n",
    "\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, final = encoder(t, torch.from_numpy(np.array([2,2,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[-1].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "We will always use teacher forcing\n",
    "\n",
    "Encoder Final is the last hidden state used to initalize first hidden state of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # will concat the previous word embedding with the context\n",
    "        # where the context is the weighted sum of the encoder hidden states\n",
    "        # which has a size of 2 * hidden\n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                         batch_first=True, dropout=dropout)\n",
    "        # to initalize from the final encoder state\n",
    "        # need to project because goes from bi-directional size to one direction\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                         hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Preform a single decoder step - 1 word\"\"\"\n",
    "        \n",
    "        # this is the hidden state for the decoder\n",
    "        query = hidden[-1].unsqueeze(1) # takes the last layer. shape: [batch, 1, hidden_dimension]\n",
    "        context, _ = self.attention(query=query, proj_key=proj_key,\n",
    "                                            value=encoder_hidden, mask=src_mask)\n",
    "        \n",
    "        # process the concat of the prev_embedding and the context (attention)\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # use the prev_embed, output from GRU, and context to get final vector of hidden_size\n",
    "        # which will be used to generated probabilities across our vocabulary\n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "        \n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, src_mask, trg_mask,\n",
    "               hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the encoder 1 step at a time\"\"\"\n",
    "        \n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "            \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "            \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        # projects them to hidden_dimension size\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(prev_embed,\n",
    "                                                          encoder_hidden,\n",
    "                                                          src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "            \n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors # [B,N,D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the intial decoder state, conditioned on final encoder state\"\"\"\n",
    "        if encoder_final is None:\n",
    "            return None #start with zeros\n",
    "        else:\n",
    "            return torch.tanh(self.bridge(encoder_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "At every time step, decoder has **ALL** the source word hidden states. Attention allows it to learn which are the most relevant. \n",
    "\n",
    "The state of the decoder is represented by the hidden state.\n",
    "\n",
    "We will use an MLP-based, additive attention with tanh activation.\n",
    "\n",
    "**Decoder state:** is the query\n",
    "**The encoder states:** the key\n",
    "\n",
    "We add the query to all of the keys and pass them through a tanh function then project them to a single number (the **energy**).\n",
    "\n",
    "We then mask out invalid positions and apply softmax to get probability distribution across the words.\n",
    "\n",
    "We then take a weighted sum of the encoder hidden states, where the weights are the probabilities. This is the **context.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # assume bi-directional encoder \n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "        \n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        #assert mask is not None, \"mask is required\"\n",
    "        \n",
    "        # project the query to hidden_size (this already done for keys)\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate energies\n",
    "        # pytorch broadcasts the query to all the keys with the addition\n",
    "        # shape batch x seq_length x 1\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        # batch x 1 x seq_length\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        # the mask is broadcastable b/c it is of size seq_length\n",
    "        # see: https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics\n",
    "        # the softmax will drop all negative infinity values\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # turn scores into probabilities\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        # context is weighted sum of the values (the original encoder hidden states)\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: batch x 1 x key size\n",
    "        # alpha shape: batch x 1 x mask length\n",
    "        return context, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do\n",
    "need to figure out how masking is working and the value in attention. The should be the same dimensions for bmm to work...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = BahdanauAttention(5)\n",
    "keys = attn.key_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.FloatTensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1])\n",
      "torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "attn(query, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 5])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.Tensor([0, float('-inf')]), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
