{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 8\n",
    "max_frames       = 10000000\n",
    "lr               = 2.5e-4\n",
    "num_steps        = 128\n",
    "mini_batch_size  = 4\n",
    "ppo_epochs       = 3\n",
    "save_every_update = 10\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "critic_weight = 1.0\n",
    "entropy_weight = 0.01\n",
    "clip_param = 0.1\n",
    "num_stack = 4\n",
    "\n",
    "save_file = \"./models/ppo_pong_0.state\"\n",
    "resume = False\n",
    "resume_file = \"./models/ppo_pong_0.state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: We detected that you are running inside a Ipython/Jupyter notebook environment but we cannot save your notebook source code. Please be sure to have installed comet_ml as a notebook server extension by running:\n",
      "jupyter comet_ml enable\n",
      "COMET INFO: old comet version (1.0.9) detected. current: 1.0.20 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET WARNING: Comet.ml support for Ipython Notebook is limited at the moment, automatic monitoring and stdout capturing is deactivated\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/syrios/ppo-pong/321206f0128a42118c56773493ac1eca\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"dZm2UV8sODS5eDYysEf8TzKNu\", \n",
    "                        project_name=\"PPO_PONG\")\n",
    "experiment.log_multiple_params({\"Num_Actors\": num_envs,\n",
    "                               \"Max_Frames\": max_frames,\n",
    "                               \"LR\": lr,\n",
    "                               \"Horizon\": num_steps,\n",
    "                               \"Num_Mini_Batches\": mini_batch_size,\n",
    "                               \"Epochs\": ppo_epochs,\n",
    "                               \"Gamma\": gamma,\n",
    "                               \"Tau\": tau,\n",
    "                               \"VF Coeff\": critic_weight,\n",
    "                               \"Entropy Coef\": entropy_weight,\n",
    "                               \"Clip Para\": clip_param,\n",
    "                               \"Num Stack\": num_stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        init_ = lambda m: init(m,\n",
    "                      nn.init.orthogonal_,\n",
    "                      lambda x: nn.init.constant_(x, 0),\n",
    "                      nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            init_(nn.Conv2d(num_inputs, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            init_(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            init_(nn.Conv2d(64, 32, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            init_(nn.Linear(32 * 7 * 7, 512)),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        init_ = lambda m: init(m,\n",
    "                               nn.init.orthogonal_,\n",
    "                               lambda x: nn.init.constant_(x, 0))\n",
    "        \n",
    "        self.critic_linear = init_(nn.Linear(512, 1))\n",
    "        self.actor_linear   = init_(nn.Linear(512, num_outputs))\n",
    "                         \n",
    "    def forward(self, x):\n",
    "        features = self.main(x / 255.0)\n",
    "        value = self.critic_linear(features)\n",
    "        softs = F.softmax(self.actor_linear(features), dim=1)\n",
    "        dist = Categorical(softs)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    current_obs = torch.zeros(num_envs, *obs_shape).to(device)\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        update_current_obs(np.expand_dims(state, axis=0))\n",
    "        dist, _ = model(current_obs)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=gamma, tau=tau):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = (3 * num_stack, 84, 84)\n",
    "\n",
    "def update_current_obs(obs):\n",
    "    shape_dim0 = 3\n",
    "    obs = obs.transpose(0, 3, 1, 2)\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    obs = nn.functional.interpolate(obs, (84,84))\n",
    "    if num_stack > 1:\n",
    "        current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "        current_obs[:, -shape_dim0:] = obs\n",
    "\n",
    "num_outputs = envs.action_space.n\n",
    "\n",
    "model = ActorCritic(3 * num_stack, num_outputs).to(device)\n",
    "if resume:\n",
    "    model.load_state_dict(torch.load(resume_file))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx    = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important\n",
    "\n",
    "With GAE: GAE advantage just used for actor loss. Discounted returns used for critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grab random values for each batch\n",
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage, values):\n",
    "    batch_size = states.size(0)\n",
    "    sampler = BatchSampler(SubsetRandomSampler(range(batch_size)), \n",
    "                           batch_size // mini_batch_size, \n",
    "                           drop_last=False)\n",
    "    for rand_ids in sampler:\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :], values[rand_ids, :]\n",
    "        \n",
    "        \n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, values, clip_param=clip_param):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for s, act, lp, r, adv, v in ppo_iter(mini_batch_size, states, actions, log_probs,\n",
    "                                                                       returns, advantages, values):\n",
    "            \n",
    "            \n",
    "            new_dist, new_value = model(s)\n",
    "            new_entropy = new_dist.entropy().mean()\n",
    "            new_log_prob = new_dist.log_prob(act)\n",
    "            ratio = torch.exp(new_log_prob - lp)\n",
    "            surr1 = ratio * adv\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * adv\n",
    "            actor_loss = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = F.mse_loss(r,v)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = (actor_loss + critic_weight * critic_loss - entropy_weight * new_entropy)\n",
    "            loss.backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Iteration: 5030\n",
      "Frames Processed: 643968\n",
      "Last Test Reward: -21.0\n",
      "Saving Model...\n"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "update_iter = 0\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    states = torch.zeros(num_steps, num_envs, *obs_shape).to(device)\n",
    "    actions = []\n",
    "    current_obs = torch.zeros(num_envs, *obs_shape).to(device)\n",
    "    \n",
    "    for step_index in range(num_steps):\n",
    "        # state is 16 x 4 because 16 envs\n",
    "        update_current_obs(state)\n",
    "        states[step_index].copy_(current_obs)\n",
    "        # dist and value each have 16 for all envs\n",
    "        dist, value = model(current_obs)\n",
    "        \n",
    "        # have 16 actions\n",
    "        action = dist.sample()\n",
    "        actions.append(action)\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        # there are 16 rewards. Need to make it 16x1. Same for masks\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "                \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "\n",
    "    update_current_obs(next_state)\n",
    "    _, next_value = model(current_obs)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    log_probs = torch.cat(log_probs).unsqueeze(1).to(device)\n",
    "    returns = torch.cat(returns).detach().to(device)\n",
    "    values = torch.cat(values).to(device)\n",
    "    advantages = returns - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
    "    actions   = torch.cat(actions).unsqueeze(1).to(device)\n",
    "    states = states.view(-1, 3*num_stack, 84, 84)\n",
    "                        \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, values)\n",
    "    \n",
    "    test_reward = test_env(False)\n",
    "    experiment.log_metric(\"reward\", test_reward,\n",
    "                          step=update_iter)\n",
    "    \n",
    "    if update_iter % save_every_update == 0:\n",
    "            clear_output(True)\n",
    "            print(\"Update Iteration: {}\".format(update_iter))\n",
    "            print(\"Frames Processed: {}\".format(frame_idx))\n",
    "            print(\"Last Test Reward: {}\".format(test_reward))\n",
    "            print(\"Saving Model...\")\n",
    "            torch.save(model.state_dict(), save_file)\n",
    "            \n",
    "    update_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
