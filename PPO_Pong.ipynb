{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/tyler/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "from baselines import bench\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "import glob\n",
    "import time\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 8\n",
    "lr               = 2.5e-4\n",
    "num_steps        = 128\n",
    "mini_batch_num   = 4\n",
    "ppo_epochs       = 4\n",
    "save_every_update = 10\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "critic_weight = 1.0\n",
    "entropy_weight = 0.01\n",
    "clip_param = 0.1\n",
    "max_frames       = 10e6 // num_envs\n",
    "\n",
    "save_file = \"./models/ppo_pong_1.state\"\n",
    "resume = \n",
    "render = False\n",
    "resume_file = \"./models/ppo_pong_0.state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: We detected that you are running inside a Ipython/Jupyter notebook environment but we cannot save your notebook source code. Please be sure to have installed comet_ml as a notebook server extension by running:\n",
      "jupyter comet_ml enable\n",
      "COMET INFO: old comet version (1.0.9) detected. current: 1.0.22 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET WARNING: Comet.ml support for Ipython Notebook is limited at the moment, automatic monitoring and stdout capturing is deactivated\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/syrios/ppo-pong/e701b7157f694772bce56c6f7d0a6cc6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"dZm2UV8sODS5eDYysEf8TzKNu\", \n",
    "                        project_name=\"PPO_PONG\")\n",
    "experiment.log_multiple_params({\"Num_Actors\": num_envs,\n",
    "                               \"Max_Frames\": max_frames,\n",
    "                               \"LR\": lr,\n",
    "                               \"Horizon\": num_steps,\n",
    "                               \"Num_Mini_Batches\": mini_batch_num,\n",
    "                               \"Epochs\": ppo_epochs,\n",
    "                               \"Gamma\": gamma,\n",
    "                               \"Tau\": tau,\n",
    "                               \"VF Coeff\": critic_weight,\n",
    "                               \"Entropy Coef\": entropy_weight,\n",
    "                               \"Clip Para\": clip_param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, rank):\n",
    "    def _thunk():\n",
    "        if env_id.startswith(\"dm\"):\n",
    "            _, domain, task = env_id.split('.')\n",
    "            env = dm_control2gym.make(domain_name=domain, task_name=task)\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "            env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "        if is_atari:\n",
    "            # Does some pre-processing that affects pong\n",
    "            # 1) Samples initial states by taking random number of no-ops\n",
    "            #    on reset (at most 30)\n",
    "            # 2) Returns only every 4th frame\n",
    "            env = make_atari(env_id)\n",
    "        env.seed(seed + rank)\n",
    "\n",
    "\n",
    "        if is_atari:\n",
    "            # Does some pre-processing that affects pong\n",
    "            # 1) Make end-of-life == end-of-episode, but only reset on true game over\n",
    "            # 2) Warp frames to 84x84\n",
    "            # 3) Stack 4 frames\n",
    "            # 4) Scale the image by dividing by 255\n",
    "            env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "\n",
    "        #wrap for PyTorch convolutions\n",
    "        env = WrapPyTorch(env)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0, 0, 0],\n",
    "            self.observation_space.high[0, 0, 0],\n",
    "            [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).astype(np.float32).transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "seed = 42\n",
    "\n",
    "\n",
    "envs = [make_env(env_name, seed, i)\n",
    "        for i in range(num_envs)]\n",
    "\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env = make_atari(env_name)\n",
    "env.seed(seed)\n",
    "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "env = WrapPyTorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        init_ = lambda m: init(m,\n",
    "                      nn.init.orthogonal_,\n",
    "                      lambda x: nn.init.constant_(x, 0),\n",
    "                      nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            init_(nn.Conv2d(num_inputs, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            init_(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            init_(nn.Conv2d(64, 32, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            init_(nn.Linear(32 * 7 * 7, 512)),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        init_ = lambda m: init(m,\n",
    "                               nn.init.orthogonal_,\n",
    "                               lambda x: nn.init.constant_(x, 0))\n",
    "        \n",
    "        self.critic_linear = init_(nn.Linear(512, 1))\n",
    "        self.actor_linear   = init_(nn.Linear(512, num_outputs))\n",
    "                         \n",
    "    def forward(self, x):\n",
    "        features = self.main(x)\n",
    "        value = self.critic_linear(features)\n",
    "        softs = F.softmax(self.actor_linear(features), dim=1)\n",
    "        dist = Categorical(softs)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=gamma, tau=tau):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = envs.action_space.n\n",
    "\n",
    "model = ActorCritic(4, num_outputs).to(device)\n",
    "if resume:\n",
    "    model.load_state_dict(torch.load(resume_file))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx    = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important\n",
    "\n",
    "With GAE: GAE advantage just used for actor loss. Discounted returns used for critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grab random values for each batch\n",
    "def ppo_iter(mini_batch_num, states, actions, log_probs, returns, advantage, values):\n",
    "    batch_size = states.size(0)\n",
    "    sampler = BatchSampler(SubsetRandomSampler(range(batch_size)), \n",
    "                           batch_size // mini_batch_num, \n",
    "                           drop_last=False)\n",
    "    for rand_ids in sampler:\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :], values[rand_ids, :]\n",
    "        \n",
    "        \n",
    "def ppo_update(ppo_epochs, mini_batch_num, states, actions, log_probs, returns, advantages, values, clip_param=clip_param):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for s, act, lp, r, adv, v in ppo_iter(mini_batch_num, states, actions, log_probs,\n",
    "                                                                       returns, advantages, values):\n",
    "            \n",
    "            \n",
    "            act = act.squeeze(1)\n",
    "            lp = lp.squeeze(1)\n",
    "            adv = adv.squeeze(1)\n",
    "            \n",
    "            new_dist, new_value = model(s)\n",
    "            new_entropy = new_dist.entropy().mean()\n",
    "            new_log_prob = new_dist.log_prob(act)            \n",
    "            ratio = torch.exp(new_log_prob - lp)\n",
    "            \n",
    "            surr1 = ratio * adv\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param,\n",
    "                                           1.0 + clip_param) * adv\n",
    "            \n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = F.mse_loss(r,new_value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = (actor_loss + critic_weight * critic_loss - entropy_weight * new_entropy)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "    return actor_loss, critic_loss, -new_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Iteration: 9760\n",
      "Frames Processed: 1249408\n",
      "Last Actor Loss: -0.051298707723617554\n",
      "Last Critic Loss: 0.13281570374965668\n",
      "Last Entropy Loss: -1.7531728744506836\n",
      "Last Test Reward: -21.0\n",
      "Saving Model...\n"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "update_iter = 0\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    \n",
    "    for step_index in range(num_steps):\n",
    "        # state is 16 x 4 because 16 envs\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        states.append(state)\n",
    "        # dist and value each have 16 for all envs\n",
    "        dist, value = model(state)\n",
    "        \n",
    "        # have 16 actions\n",
    "        action = dist.sample()\n",
    "        actions.append(action)\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        # there are 16 rewards. Need to make it 16x1. Same for masks\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "                \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    log_probs = torch.cat(log_probs).unsqueeze(1).to(device)\n",
    "    returns = torch.cat(returns).detach().to(device)\n",
    "    values = torch.cat(values).to(device)\n",
    "    advantages = returns - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
    "    actions   = torch.cat(actions).unsqueeze(1).to(device)\n",
    "    states = torch.cat(states).to(device)\n",
    "                        \n",
    "    last_al, last_cl, last_el = ppo_update(ppo_epochs, mini_batch_num, states, actions, \n",
    "                log_probs, returns, advantages, values)\n",
    "    \n",
    "    test_reward = test_env(render)\n",
    "    experiment.log_multiple_metrics({\"reward\": test_reward,\n",
    "                                     \"last al\": last_al,\n",
    "                                     \"last cl\": last_cl,\n",
    "                                     \"last el\": last_el}, step=update_iter)\n",
    "    \n",
    "    if update_iter % save_every_update == 0:\n",
    "            clear_output(True)\n",
    "            print(\"Update Iteration: {}\".format(update_iter))\n",
    "            print(\"Frames Processed: {}\".format(frame_idx))\n",
    "            print(\"Last Actor Loss: {}\".format(last_al))\n",
    "            print(\"Last Critic Loss: {}\".format(last_cl))\n",
    "            print(\"Last Entropy Loss: {}\".format(last_el))\n",
    "            print(\"Last Test Reward: {}\".format(test_reward))\n",
    "            print(\"Saving Model...\")\n",
    "            torch.save(model.state_dict(), save_file)\n",
    "            \n",
    "    update_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
