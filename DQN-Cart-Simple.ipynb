{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import models\n",
    "from comet_ml import Experiment\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(api_key=\"dZm2UV8sODS5eDYysEf8TzKNu\", project_name=\"cart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"Pong-v0\")\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "Store transitions that the agent observes so can be re-used later. By sampling from this randomly improves stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                       ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.0\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 1000000 + 1000\n",
    "TARGET_UPDATE = 10000\n",
    "n_frames_min = 2000000\n",
    "SAVE_EVERY = 250\n",
    "REPLAY_SIZE = 100000\n",
    "LR = 1e-4\n",
    "BURN_IN = 1000\n",
    "RENDER = True\n",
    "model_save_name = \"./models/cart_dqn_simple_3.state\"\n",
    "start_from_trained = True\n",
    "trained_model = \"./models/cart_dqn_simple_2.state\"\n",
    "N_ACTIONS = 2\n",
    "\n",
    "experiment.log_parameter(\"batch size\", BATCH_SIZE)\n",
    "experiment.log_parameter(\"gamma\", GAMMA)\n",
    "experiment.log_parameter(\"eps start\", EPS_START)\n",
    "experiment.log_parameter(\"eps end\", EPS_END)\n",
    "experiment.log_parameter(\"eps decay\", EPS_DECAY)\n",
    "experiment.log_parameter(\"target update\", TARGET_UPDATE)\n",
    "experiment.log_parameter(\"min frames\", n_frames_min)\n",
    "experiment.log_parameter(\"start trained\", start_from_trained)\n",
    "experiment.log_parameter(\"replay size\", REPLAY_SIZE)\n",
    "experiment.log_parameter(\"learning rate\", LR)\n",
    "experiment.log_parameter(\"burn in\", BURN_IN)\n",
    "\n",
    "\n",
    "# the policy network is used to play the game - aka actor\n",
    "policy_net = models.NNPolicy().cuda()\n",
    "if start_from_trained:\n",
    "    policy_net.load_state_dict(torch.load(trained_model))\n",
    "\n",
    "# the target net is used to predict Q values for next action\n",
    "# we need 2 otherwise we would be using the same network\n",
    "# in the actual and predicted values of our loss function\n",
    "target_net = models.NNPolicy().cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "# sets training to false \n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(REPLAY_SIZE)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    # gen random number\n",
    "    sample = random.random()\n",
    "    # get threshold which decays from start to end \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    # if exceed, pick best\n",
    "    if sample > eps_threshold:\n",
    "        # No gradients b/c not learning, just getting best one\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1,1), eps_threshold\n",
    "    # else, random\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device='cuda', \n",
    "                            dtype=torch.long), eps_threshold  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE or len(memory) < BURN_IN:\n",
    "        return None\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # transpose the batch...\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # compute mask of transitions which didn't lead to ending game\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=\"cuda\", dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # calculate the Q value of taking the state, action pairs which were taken\n",
    "    # the gather basically takes the Q value for the action choosen\n",
    "    # So if my input choose action 3 for the given state, that is what I would gather.\n",
    "    # Basically, what is the Q value for what actually happened\n",
    "    # Q value being the total expected value from taking an action given a state.\n",
    "    # These are basically our predictions for learning\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE).cuda()\n",
    "    # get the best actions for the next states\n",
    "    # detach is for speed so don't calc gradients\n",
    "    target_net_results = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    next_state_values[non_final_mask] = target_net_results\n",
    "    # the expected value of the Q(s,a) given from the policy net is the\n",
    "    # reward given plus the discounted value of the Q value from taking the best\n",
    "    # action at the next step\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, \n",
    "                            expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the policy net to become better and predicting Q values\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_reward = None\n",
    "reward_sum = 0\n",
    "n_frames = 0\n",
    "i_episode = 0\n",
    "while n_frames < n_frames_min:\n",
    "    obs = env.reset()\n",
    "    state = torch.from_numpy(obs).float().unsqueeze(0).cuda()\n",
    "    losses = []\n",
    "    for t in count():\n",
    "        \n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        action, eps_threshold = select_action(state)\n",
    "        obs, reward, done, _ = env.step(action.item())\n",
    "        reward_sum += reward\n",
    "        reward = torch.tensor([reward])\n",
    "        reward = reward.cuda()\n",
    "\n",
    "        n_frames += 1\n",
    "        if n_frames % TARGET_UPDATE == 0:\n",
    "            print(\"Frame: {}\".format(n_frames))\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        if not done:\n",
    "            next_state = torch.from_numpy(obs).float().unsqueeze(0).cuda()\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print(reward_sum)\n",
    "            experiment.log_metric(\"reward sum\", reward_sum, step=i_episode)\n",
    "            reward_sum = 0\n",
    "            experiment.log_metric(\"reward mean\", running_reward, step=i_episode)\n",
    "            experiment.log_metric(\"eps\", eps_threshold, step=i_episode)\n",
    "            i_episode += 1\n",
    "            break\n",
    "\n",
    "    if i_episode % SAVE_EVERY == 0:\n",
    "        print(\"Episode: {}\".format(i_episode))\n",
    "        print(\"Saving Model...\")\n",
    "        torch.save(policy_net.state_dict(), model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
