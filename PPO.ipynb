{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/tyler/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from baselines import bench\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 8\n",
    "env_name = \"CartPole-v0\"\n",
    "seed = 42\n",
    "\n",
    "def make_env(env_id, seed, rank):\n",
    "    def _thunk():\n",
    "        if env_id.startswith(\"dm\"):\n",
    "            _, domain, task = env_id.split('.')\n",
    "            env = dm_control2gym.make(domain_name=domain, task_name=task)\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "            env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "        if is_atari:\n",
    "            # Does some pre-processing that affects pong\n",
    "            # 1) Samples initial states by taking random number of no-ops\n",
    "            #    on reset (at most 30)\n",
    "            # 2) Returns only every 4th frame\n",
    "            env = make_atari(env_id)\n",
    "        env.seed(seed + rank)\n",
    "\n",
    "\n",
    "        if is_atari:\n",
    "            # Does some pre-processing that affects pong\n",
    "            # 1) Make end-of-life == end-of-episode, but only reset on true game over\n",
    "            # 2) Warp frames to 84x84\n",
    "            # 3) Stack 4 frames\n",
    "            # 4) Scale the image by dividing by 255\n",
    "            env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "\n",
    "envs = [make_env(env_name, seed, i)\n",
    "        for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normc_(weight, gain=1):\n",
    "    weight.normal_(0, 1)\n",
    "    weight *= gain / torch.sqrt(weight.pow(2).sum(1, keepdim=True))\n",
    "    \n",
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        init_ = lambda m: init(m,\n",
    "              init_normc_,\n",
    "              lambda x: nn.init.constant_(x, 0))\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            init_(nn.Linear(num_inputs, 64)),\n",
    "            nn.Tanh(),\n",
    "            init_(nn.Linear(64, 64)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            init_(nn.Linear(num_inputs, 64)),\n",
    "            nn.Tanh(),\n",
    "            init_(nn.Linear(64, 64)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.critic_linear = init_(nn.Linear(64, 1))\n",
    "        self.actor_linear   = init_(nn.Linear(64, num_outputs))\n",
    "                         \n",
    "    def forward(self, x):\n",
    "        value = self.critic_linear(self.critic(x))\n",
    "        softs = F.softmax(self.actor_linear(self.actor(x)), dim=1)\n",
    "        dist = Categorical(softs)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.n\n",
    "\n",
    "#Hyper params:\n",
    "max_frames       = 5000\n",
    "lr               = 2.5e-4\n",
    "num_steps        = 128\n",
    "mini_batch_size  = 32\n",
    "ppo_epochs       = 4\n",
    "print_every      = 500\n",
    "\n",
    "model = ActorCritic(4, num_outputs).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx    = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important\n",
    "\n",
    "With GAE: GAE advantage just used for actor loss. Discounted returns used for critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grab random values for each batch\n",
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage, values):\n",
    "    batch_size = states.size(0)\n",
    "    print(actions.shape)\n",
    "    sampler = BatchSampler(SubsetRandomSampler(range(batch_size)), \n",
    "                           batch_size // mini_batch_size, \n",
    "                           drop_last=False)\n",
    "    for rand_ids in sampler:\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :], values[rand_ids, :]\n",
    "        \n",
    "        \n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, values, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for s, act, old_log_probs, r, advantage, v in ppo_iter(mini_batch_size, states, actions, log_probs,\n",
    "                                                                       returns, advantages, values):\n",
    "            dist, value = model(s)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(act)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = F.mse_loss(r,value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = (actor_loss + 0.5 * critic_loss - 0.01 * entropy)\n",
    "            loss.backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAE/CAYAAAC9y4P/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cc2bdb255962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mactions\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mppo_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppo_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-d7db218c508a>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, values, clip_param)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppo_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         for s, act, old_log_probs, r, advantage, v in ppo_iter(mini_batch_size, states, actions, log_probs,\n\u001b[0;32m---> 15\u001b[0;31m                                                                        returns, advantages, values):\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d7db218c508a>\u001b[0m in \u001b[0;36mppo_iter\u001b[0;34m(mini_batch_size, states, actions, log_probs, returns, advantage, values)\u001b[0m\n\u001b[1;32m      7\u001b[0m                            drop_last=False)\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrand_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        # state is 16 x 4 because 16 envs\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        states.append(state)\n",
    "        # dist and value each have 16 for all envs\n",
    "        dist, value = model(state)\n",
    "        \n",
    "        # have 16 actions\n",
    "        action = dist.sample()\n",
    "        actions.append(action)\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        # there are 16 rewards. Need to make it 16x1. Same for masks\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "                \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % print_every == 0:\n",
    "            test_rewards.append(np.mean([test_env() for _ in range(10)]))\n",
    "            plot(frame_idx, test_rewards)\n",
    "            \n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    log_probs = torch.cat(log_probs).unsqueeze(1).to(device)\n",
    "    returns = torch.cat(returns).detach().to(device)\n",
    "    values = torch.cat(values).to(device)\n",
    "    advantages = returns - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
    "    states    = torch.cat(states).to(device)\n",
    "    actions   = torch.cat(actions).unsqueeze(1).to(device)\n",
    "                        \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
